{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504747f1-3811-4ecb-8b45-231e4f42301b",
   "metadata": {},
   "source": [
    "source: https://github.com/dallascard/us-immigration-speeches/blob/main/parsing/parse_hein_bound.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fadd39-7bcd-4885-b715-4411d3cc70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from optparse import OptionParser\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff862dd-cd63-4b64-aada-19977a627857",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed/'\n",
    "first = 110\n",
    "last = 111\n",
    "encoding = 'Windows-1252'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51417931-291c-4452-9298-1e859371575c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-06 18:43:08.091585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-06 18:43:08.091614: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 850 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.9.14)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "Installing collected packages: en-core-web-sm\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "Successfully installed en-core-web-sm-3.3.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7a1e5-e633-4761-bdcb-d0e6567785cf",
   "metadata": {},
   "source": [
    "# parse_hein_bound.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c91cc4-687b-4210-9586-ae4e817e8ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 212387/212387 [1:08:20<00:00, 51.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 212386 lines to ./Congress/hein-bound_parsed/speeches_110.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 179269/179269 [1:00:58<00:00, 49.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 179268 lines to ./Congress/hein-bound_parsed/speeches_111.txt\n"
     ]
    }
   ],
   "source": [
    "usage = \"%prog\"\n",
    "parser = OptionParser(usage=usage)\n",
    "# parser.add_option('--indir', type=str, default='./Congress/hein-bound/',\n",
    "#                   help='Hein bound dir: default=%default')\n",
    "# parser.add_option('--outdir', type=str, default='./Congress/hein-bound_parsed/',\n",
    "#                   help='Hein bound dir: default=%default')\n",
    "# parser.add_option('--first', type=int, default=43,\n",
    "#                   help='First congress: default=%default')\n",
    "# parser.add_option('--last', type=int, default=111,\n",
    "#                   help='Last congress: default=%default')\n",
    "# parser.add_option('--encoding', type=str, default='Windows-1252',\n",
    "#                   help='Infile encoding: default=%default')\n",
    "#parser.add_option('--by-issue', action=\"store_true\", default=False,\n",
    "#                  help='Divide data by issue: default=%default')\n",
    "\n",
    "# (options, args) = parser.parse_args()\n",
    "\n",
    "# indir = options.indir\n",
    "# outdir = options.outdir\n",
    "# first = options.first\n",
    "# last = options.last\n",
    "# encoding = options.encoding\n",
    "\n",
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed/'\n",
    "first = 110\n",
    "last = 111\n",
    "encoding = 'Windows-1252'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "print(\"Loading spacy\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for congress in range(first, last+1):\n",
    "    outfile = os.path.join(outdir, 'speeches_' + str(congress).zfill(3) + '.txt')\n",
    "    if os.path.isfile(outfile):\n",
    "        print(str(congress)+\" existed\")\n",
    "    else:\n",
    "        \n",
    "        infile = os.path.join(indir, 'speeches_' + str(congress).zfill(3) + '.txt')\n",
    "        descr_file = os.path.join(indir, 'descr_' + str(congress).zfill(3) + '.txt')\n",
    "        basename = os.path.splitext(os.path.basename(infile))[0]\n",
    "\n",
    "        # first read the description file to get speech dates\n",
    "        speech_dates = {}\n",
    "        with open(descr_file, encoding=encoding) as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.split('|')\n",
    "            speech_id = parts[0]\n",
    "            date = parts[2]\n",
    "            speech_dates[speech_id] = date\n",
    "\n",
    "        with open(infile, encoding='Windows-1252') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        outlines = []\n",
    "        for line in tqdm(lines):\n",
    "            line = line.strip()\n",
    "            parts = line.split('|')\n",
    "            line_id = parts[0]\n",
    "            # drop the header\n",
    "            if line_id != 'speech_id':\n",
    "                date = speech_dates[line_id]\n",
    "                # skip one day that has is corrupted by data from 1994\n",
    "                if date != '18940614':\n",
    "                    text = ' '.join(parts[1:])\n",
    "\n",
    "                    # parse the text\n",
    "                    parsed = nlp(text)\n",
    "\n",
    "                    # reattach possessive 's's\n",
    "                    possessives = [token.i for token in parsed if token.tag_ == 'POS' and token.text == \"'s\" and token.sent.start != token.i]\n",
    "                    with parsed.retokenize() as retokenizer:\n",
    "                        for pos in possessives:\n",
    "                            retokenizer.merge(parsed[pos-1:pos+1])\n",
    "\n",
    "                    # collect features to be saved\n",
    "                    sents = []\n",
    "                    tokens = []\n",
    "                    lemmas = []\n",
    "                    tags = []\n",
    "                    deps = []\n",
    "                    heads = []\n",
    "                    for sent in parsed.sents:\n",
    "                        sents.append(sent.text)\n",
    "                        tokens.append([token.text for token in sent])\n",
    "                        lemmas.append([token.lemma_ for token in sent])\n",
    "                        tags.append([token.tag_ for token in sent])\n",
    "                        deps.append([token.dep_ for token in sent])\n",
    "                        heads.append([token.head.i - sent.start for token in sent])\n",
    "\n",
    "                    outlines.append({'id': line_id, 'tokens': tokens, 'lemmas': lemmas, 'tags': tags, 'deps': deps, 'heads': heads})\n",
    "\n",
    "\n",
    "        print(\"Saving {:d} lines to {:s}\".format(len(outlines), outfile))\n",
    "        with open(outfile, 'w') as fo:\n",
    "            for line in outlines:\n",
    "                fo.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c6c74",
   "metadata": {},
   "source": [
    "# parse by party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70778cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>party</th>\n",
       "      <th>party_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>anti-administration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P</td>\n",
       "      <td>pro-administration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>federalist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>democratic republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>113</td>\n",
       "      <td>D</td>\n",
       "      <td>democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>113</td>\n",
       "      <td>I</td>\n",
       "      <td>independent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>114</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>114</td>\n",
       "      <td>D</td>\n",
       "      <td>democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>114</td>\n",
       "      <td>I</td>\n",
       "      <td>independent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     session party             party_full\n",
       "0          1     A    anti-administration\n",
       "1          1     P     pro-administration\n",
       "2          1     F             federalist\n",
       "3          1     R             republican\n",
       "4          1     D  democratic republican\n",
       "..       ...   ...                    ...\n",
       "542      113     D               democrat\n",
       "543      113     I            independent\n",
       "544      114     R             republican\n",
       "545      114     D               democrat\n",
       "546      114     I            independent\n",
       "\n",
       "[547 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party_full = './Congress/party_full.txt'\n",
    "\n",
    "\n",
    "# Create a DataFrame from the sample data\n",
    "dfparty = pd.read_csv(party_full, sep='|')\n",
    "dfparty\n",
    "# # Function to categorize party based on 'party_full' content\n",
    "# def categorize_party(party_full):\n",
    "#     if 'republican' in party_full.lower().split(' ')[-1]:\n",
    "#         return 'Republican'\n",
    "#     elif 'democrat' in party_full.lower().split(' ')[-1]:\n",
    "#         return 'Democrat'\n",
    "#     else:\n",
    "#         return 'Other'\n",
    "\n",
    "# # Apply the categorization function to the 'party_full' column\n",
    "# dfparty['party_category'] = dfparty['party_full'].apply(categorize_party)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a4ebd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "party\n",
       "D    109\n",
       "R    107\n",
       "I     82\n",
       "A     50\n",
       "P     37\n",
       "F     35\n",
       "U     34\n",
       "W     20\n",
       "J     17\n",
       "C     17\n",
       "N     17\n",
       "S     16\n",
       "L      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfparty['party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a88a0594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "party_full\n",
       "independent                   40\n",
       "unionist                      23\n",
       "whig                          20\n",
       "federalist                    19\n",
       "farmer-labor                  11\n",
       "adams                          9\n",
       "jackson                        9\n",
       "progressive                    9\n",
       "socialist                      8\n",
       "american labor                 7\n",
       "american                       7\n",
       "conservative                   6\n",
       "populist                       6\n",
       "anti masonic                   6\n",
       "coalitionist                   6\n",
       "jacksonian                     6\n",
       "new progressive                6\n",
       "nullifier                      5\n",
       "national greenbacker           5\n",
       "anti-jacksonian                5\n",
       "anti jacksonian                5\n",
       "anti-administration            4\n",
       "free soil                      4\n",
       "unconditional unionist         3\n",
       "pro-administration             3\n",
       "unknown                        3\n",
       "silver                         3\n",
       "anti mason                     3\n",
       "prohibitionist                 3\n",
       "union                          2\n",
       "union labor                    2\n",
       "ind. whig                      2\n",
       "states rights                  1\n",
       "anti jackson                   1\n",
       "liberal                        1\n",
       "nonpartisan                    1\n",
       "democratic and union labor     1\n",
       "law and order                  1\n",
       "liberty                        1\n",
       "constitutional unionist        1\n",
       "free silver                    1\n",
       "readjuster                     1\n",
       "al                             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the party_full when category is Other\n",
    "df[df['party_category'] == 'Other']['party_full'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "245282d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m speech_id, party_code \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m1\u001b[39m], parts[\u001b[38;5;241m7\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m party_full_name \u001b[38;5;241m=\u001b[39m dfparty[dfparty[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparty\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m party_code][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparty_full\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfparty[\u001b[43mdfparty\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparty_code\u001b[49m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m categorized_party \u001b[38;5;241m=\u001b[39m categorize_party(party_full_name)\n\u001b[1;32m     40\u001b[0m party_map[speech_id] \u001b[38;5;241m=\u001b[39m categorized_party\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/series.py:6112\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6108\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6110\u001b[0m res_values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m-> 6112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/series.py:6222\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   6219\u001b[0m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[1;32m   6220\u001b[0m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[1;32m   6221\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 6222\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   6223\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   6225\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   6226\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/series.py:593\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    590\u001b[0m         data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n\u001b[1;32m    592\u001b[0m NDFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_axis(\u001b[38;5;241m0\u001b[39m, index)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/generic.py:6317\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6314\u001b[0m \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[1;32m   6315\u001b[0m \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[1;32m   6316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set:\n\u001b[0;32m-> 6317\u001b[0m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata:\n\u001b[1;32m   6319\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacy/lib/python3.11/site-packages/pandas/core/series.py:780\u001b[0m, in \u001b[0;36mSeries.name\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;124;03m    Return the name of the Series.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m    'Even Numbers'\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\n\u001b[0;32m--> 780\u001b[0m \u001b[38;5;129m@name\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     validate_all_hashable(value, error_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# File paths\n",
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed_party/'\n",
    "party_full = './Congress/party_full.txt'\n",
    "first = 43\n",
    "last = 137\n",
    "encoding = 'Windows-1252'\n",
    "# Read and categorize party_full.txt\n",
    "dfparty = pd.read_csv(party_full, sep='|')\n",
    "dfparty['party_code'] = dfparty['party'].astype(str)\n",
    "# Function to categorize party based on 'party_full' content\n",
    "def categorize_party(party_full):\n",
    "    if 'republican' in party_full.lower().split(' ')[-1]:\n",
    "        return 'Republican'\n",
    "    elif 'democrat' in party_full.lower().split(' ')[-1]:\n",
    "        return 'Democrat'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the categorization function to the 'party_full' column\n",
    "dfparty['party_category'] = dfparty['party_full'].apply(categorize_party)\n",
    "\n",
    "# Initialize an empty list to collect speech metadata\n",
    "speech_metadata = []\n",
    "\n",
    "for congress in tqdm(range(first, last+1)):\n",
    "    speaker_map_file = os.path.join(indir, str(congress).zfill(3) + '_SpeakerMap.txt')\n",
    "    party_map = {}\n",
    "    speech_file = f'speeches_{str(congress).zfill(3)}.txt'  # Name of the speech file being processed\n",
    "\n",
    "    # Parse SpeakerMap to get party affiliations\n",
    "    with open(speaker_map_file, encoding=encoding) as f:\n",
    "        for line in f.readlines()[1:]:  # Skip header line\n",
    "            parts = line.strip().split('|')\n",
    "            speech_id, party_code = parts[1], parts[7]\n",
    "            party_full_name = dfparty[dfparty['party'] == party_code]['party_full'].values[0] if len(dfparty[dfparty['party'] == party_code]) > 0 else 'Other'\n",
    "            categorized_party = categorize_party(party_full_name)\n",
    "            party_map[speech_id] = categorized_party\n",
    "\n",
    "            # Collect metadata for each speech\n",
    "            speech_metadata.append({\n",
    "                'speech_id': speech_id,\n",
    "                'party_code': party_code,\n",
    "                'party_full_name': party_full_name,  # New column 'party_full_name\n",
    "                'categorized_party': categorized_party,\n",
    "                'speech_file': speech_file\n",
    "            })\n",
    "\n",
    "    # The rest of the processing remains the same\n",
    "\n",
    "# After processing, create a DataFrame from the collected metadata\n",
    "df_speech_metadata = pd.DataFrame(speech_metadata)\n",
    "\n",
    "# Display the DataFrame to verify the content\n",
    "print(df_speech_metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce150789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speakerid</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>lastname</th>\n",
       "      <th>firstname</th>\n",
       "      <th>chamber</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>district</th>\n",
       "      <th>nonvoting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46045281</td>\n",
       "      <td>460000006</td>\n",
       "      <td>WALLACE</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>S</td>\n",
       "      <td>PA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46046491</td>\n",
       "      <td>460000008</td>\n",
       "      <td>ROLLINS</td>\n",
       "      <td>EDWARD</td>\n",
       "      <td>S</td>\n",
       "      <td>NH</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46045281</td>\n",
       "      <td>460000009</td>\n",
       "      <td>WALLACE</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>S</td>\n",
       "      <td>PA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46045281</td>\n",
       "      <td>460000010</td>\n",
       "      <td>WALLACE</td>\n",
       "      <td>WILLIAM</td>\n",
       "      <td>S</td>\n",
       "      <td>PA</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46047111</td>\n",
       "      <td>460000012</td>\n",
       "      <td>CONKLING</td>\n",
       "      <td>ROSCOE</td>\n",
       "      <td>S</td>\n",
       "      <td>NY</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139459</th>\n",
       "      <td>46045781</td>\n",
       "      <td>460120688</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>S</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139460</th>\n",
       "      <td>46049820</td>\n",
       "      <td>460151464</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>H</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>R</td>\n",
       "      <td>2.0</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139461</th>\n",
       "      <td>46045781</td>\n",
       "      <td>460156619</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>S</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139462</th>\n",
       "      <td>46045781</td>\n",
       "      <td>460168207</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>S</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139463</th>\n",
       "      <td>46045781</td>\n",
       "      <td>460171847</td>\n",
       "      <td>DAVIS</td>\n",
       "      <td>DAVID</td>\n",
       "      <td>S</td>\n",
       "      <td>IL</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139464 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        speakerid  speech_id  lastname firstname chamber state gender party  \\\n",
       "0        46045281  460000006   WALLACE   WILLIAM       S    PA      M     D   \n",
       "1        46046491  460000008   ROLLINS    EDWARD       S    NH      M     R   \n",
       "2        46045281  460000009   WALLACE   WILLIAM       S    PA      M     D   \n",
       "3        46045281  460000010   WALLACE   WILLIAM       S    PA      M     D   \n",
       "4        46047111  460000012  CONKLING    ROSCOE       S    NY      M     R   \n",
       "...           ...        ...       ...       ...     ...   ...    ...   ...   \n",
       "139459   46045781  460120688     DAVIS     DAVID       S    IL      M     I   \n",
       "139460   46049820  460151464     DAVIS    GEORGE       H    IL      M     R   \n",
       "139461   46045781  460156619     DAVIS     DAVID       S    IL      M     I   \n",
       "139462   46045781  460168207     DAVIS     DAVID       S    IL      M     I   \n",
       "139463   46045781  460171847     DAVIS     DAVID       S    IL      M     I   \n",
       "\n",
       "        district nonvoting  \n",
       "0            NaN    voting  \n",
       "1            NaN    voting  \n",
       "2            NaN    voting  \n",
       "3            NaN    voting  \n",
       "4            NaN    voting  \n",
       "...          ...       ...  \n",
       "139459       NaN    voting  \n",
       "139460       2.0    voting  \n",
       "139461       NaN    voting  \n",
       "139462       NaN    voting  \n",
       "139463       NaN    voting  \n",
       "\n",
       "[139464 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.read_csv(speaker_map_file,sep = '|')\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8285157",
   "metadata": {},
   "source": [
    "## parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "827a3fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58092b8505e245a28f414ce4f0ceb413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Congress Sessions:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# File paths\n",
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed_party/'\n",
    "party_full = './Congress/party_full.txt'\n",
    "first = 43\n",
    "last = 111\n",
    "encoding = 'Windows-1252'\n",
    "# Read and categorize party_full.txt\n",
    "dfparty = pd.read_csv(party_full, sep='|')\n",
    "dfparty['party_code'] = dfparty['party'].astype(str)\n",
    "\n",
    "def process_congress(congress):\n",
    "    speaker_map_file = os.path.join(indir, f'{congress:03}_SpeakerMap.txt')\n",
    "    speech_file = f'speeches_{congress:03}.txt'\n",
    "\n",
    "    # Read SpeakerMap file into DataFrame\n",
    "    df_speaker_map = pd.read_csv(speaker_map_file, sep='|', encoding=encoding, skiprows=1, header=None,\n",
    "                                 names=['speakerid', 'speech_id', 'lastname', 'firstname', 'chamber', 'state', 'gender', 'party_code', 'district', 'nonvoting'])\n",
    "\n",
    "    # Merge with party categorization\n",
    "    df_speaker_map = df_speaker_map.merge(dfparty, how='left', on='party_code')\n",
    "    # df_speaker_map['categorized_party'] = df_speaker_map['party_full'].apply(categorize_party)\n",
    "    \n",
    "    # Add 'speech_file' column\n",
    "    df_speaker_map['speech_file'] = speech_file\n",
    "\n",
    "    return df_speaker_map[['speech_id', 'party_code', 'party_full', 'speech_file']]\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "df_speech_metadata = pd.DataFrame(columns=['speech_id', 'party_code', 'party_full', 'speech_file'])\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_congress, congress) for congress in range(first, last+1)]\n",
    "    for future in tqdm(futures, total=last-first+1, desc='Processing Congress Sessions'):\n",
    "        df_speech_metadata = pd.concat([df_speech_metadata, future.result()], ignore_index=True)\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61dbb029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata to congress_party.csv\n"
     ]
    }
   ],
   "source": [
    "# save the metadata to a file\n",
    "filename= 'congress_party.csv'\n",
    "df_speech_metadata.to_csv(os.path.join('./Congress/',filename), index=False)\n",
    "print(f\"Saved metadata to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21202cb8",
   "metadata": {},
   "source": [
    "## merge party and year for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b80ddd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech_id</th>\n",
       "      <th>year</th>\n",
       "      <th>filenum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430000001</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430000002</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>430000003</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430000004</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430000005</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395879</th>\n",
       "      <td>1110179264</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395880</th>\n",
       "      <td>1110179265</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395881</th>\n",
       "      <td>1110179266</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395882</th>\n",
       "      <td>1110179267</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395883</th>\n",
       "      <td>1110179268</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17395884 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           speech_id  year  filenum\n",
       "0          430000001  1873       43\n",
       "1          430000002  1873       43\n",
       "2          430000003  1873       43\n",
       "3          430000004  1873       43\n",
       "4          430000005  1873       43\n",
       "...              ...   ...      ...\n",
       "17395879  1110179264  2010      111\n",
       "17395880  1110179265  2010      111\n",
       "17395881  1110179266  2010      111\n",
       "17395882  1110179267  2010      111\n",
       "17395883  1110179268  2010      111\n",
       "\n",
       "[17395884 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_party = pd.read_csv('./Congress/congress_party.csv')\n",
    "df_year = pd.read_csv('./Congress/congress_year.csv')\n",
    "df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb28d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filenum 43 is associated with multiple years: [1873 1874 1875]\n",
      "Filenum 44 is associated with multiple years: [1875 1876 1877]\n",
      "Filenum 45 is associated with multiple years: [1877 1878 1879]\n",
      "Filenum 46 is associated with multiple years: [1879 1880 1881]\n",
      "Filenum 47 is associated with multiple years: [1881 1882 1883]\n",
      "Filenum 48 is associated with multiple years: [1883 1884 1885]\n",
      "Filenum 49 is associated with multiple years: [1885 1886 1887]\n",
      "Filenum 50 is associated with multiple years: [1887 1888 1889]\n",
      "Filenum 51 is associated with multiple years: [1889 1890 1891]\n",
      "Filenum 52 is associated with multiple years: [1891 1892 1893]\n",
      "Filenum 53 is associated with multiple years: [1893 1894 1895]\n",
      "Filenum 54 is associated with multiple years: [1895 1896 1897]\n",
      "Filenum 55 is associated with multiple years: [1897 1898 1899]\n",
      "Filenum 56 is associated with multiple years: [1899 1900 1901]\n",
      "Filenum 57 is associated with multiple years: [1901 1902 1903]\n",
      "Filenum 58 is associated with multiple years: [1903 1904 1905]\n",
      "Filenum 59 is associated with multiple years: [1905 1906 1907]\n",
      "Filenum 60 is associated with multiple years: [1907 1908 1909]\n",
      "Filenum 61 is associated with multiple years: [1909 1910 1911]\n",
      "Filenum 62 is associated with multiple years: [1911 1912 1913]\n",
      "Filenum 63 is associated with multiple years: [1913 1914 1915]\n",
      "Filenum 64 is associated with multiple years: [1915 1916 1917]\n",
      "Filenum 65 is associated with multiple years: [1917 1918 1919]\n",
      "Filenum 66 is associated with multiple years: [1919 1920 1921]\n",
      "Filenum 67 is associated with multiple years: [1921 1922 1923]\n",
      "Filenum 68 is associated with multiple years: [1923 1924 1925]\n",
      "Filenum 69 is associated with multiple years: [1925 1926 1927]\n",
      "Filenum 70 is associated with multiple years: [1927 1928 1929]\n",
      "Filenum 71 is associated with multiple years: [1929 1930 1931]\n",
      "Filenum 72 is associated with multiple years: [1931 1932 1933]\n",
      "Filenum 73 is associated with multiple years: [1933 1934]\n",
      "Filenum 74 is associated with multiple years: [1935 1936]\n",
      "Filenum 75 is associated with multiple years: [1937 1938]\n",
      "Filenum 76 is associated with multiple years: [1939 1940 1941]\n",
      "Filenum 77 is associated with multiple years: [1941 1942]\n",
      "Filenum 78 is associated with multiple years: [1943 1944]\n",
      "Filenum 79 is associated with multiple years: [1945 1946]\n",
      "Filenum 80 is associated with multiple years: [1947 1948]\n",
      "Filenum 81 is associated with multiple years: [1949 1950 1951]\n",
      "Filenum 82 is associated with multiple years: [1951 1952]\n",
      "Filenum 83 is associated with multiple years: [1953 1954]\n",
      "Filenum 84 is associated with multiple years: [1955 1956]\n",
      "Filenum 85 is associated with multiple years: [1957 1958]\n",
      "Filenum 86 is associated with multiple years: [1959 1960]\n",
      "Filenum 87 is associated with multiple years: [1961 1962]\n",
      "Filenum 88 is associated with multiple years: [1963 1964]\n",
      "Filenum 89 is associated with multiple years: [1965 1966]\n",
      "Filenum 90 is associated with multiple years: [1967 1968]\n",
      "Filenum 91 is associated with multiple years: [1969 1970 1971]\n",
      "Filenum 92 is associated with multiple years: [1971 1972]\n",
      "Filenum 93 is associated with multiple years: [1973 1974]\n",
      "Filenum 94 is associated with multiple years: [1975 1976]\n",
      "Filenum 95 is associated with multiple years: [1977 1978]\n",
      "Filenum 96 is associated with multiple years: [1979 1980]\n",
      "Filenum 97 is associated with multiple years: [1981 1982]\n",
      "Filenum 98 is associated with multiple years: [1983 1984]\n",
      "Filenum 99 is associated with multiple years: [1985 1986]\n",
      "Filenum 100 is associated with multiple years: [1987 1988]\n",
      "Filenum 101 is associated with multiple years: [1989 1990]\n",
      "Filenum 102 is associated with multiple years: [1991 1992]\n",
      "Filenum 103 is associated with multiple years: [1993 1994]\n",
      "Filenum 104 is associated with multiple years: [1995 1996]\n",
      "Filenum 105 is associated with multiple years: [1997 1998]\n",
      "Filenum 106 is associated with multiple years: [1999 2000]\n",
      "Filenum 107 is associated with multiple years: [2001 2002]\n",
      "Filenum 108 is associated with multiple years: [2003 2004]\n",
      "Filenum 109 is associated with multiple years: [2005 2006]\n",
      "Filenum 110 is associated with multiple years: [2007 2008 2009]\n",
      "Filenum 111 is associated with multiple years: [2009 2010]\n"
     ]
    }
   ],
   "source": [
    "grouped = df_year.groupby('filenum')['year'].unique()\n",
    "for filenum, years in grouped.items():\n",
    "    if len(years) > 1:\n",
    "        print(f\"Filenum {filenum} is associated with multiple years: {years}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e4359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59ca52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech_id</th>\n",
       "      <th>party_code</th>\n",
       "      <th>party_full</th>\n",
       "      <th>speech_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430000002</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>430000002</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>430000002</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>430000002</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>430000002</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428339766</th>\n",
       "      <td>1110179211</td>\n",
       "      <td>D</td>\n",
       "      <td>democrat</td>\n",
       "      <td>speeches_111.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428339767</th>\n",
       "      <td>1110179211</td>\n",
       "      <td>D</td>\n",
       "      <td>democrat</td>\n",
       "      <td>speeches_111.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428339768</th>\n",
       "      <td>1110179211</td>\n",
       "      <td>D</td>\n",
       "      <td>democrat</td>\n",
       "      <td>speeches_111.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428339757</th>\n",
       "      <td>1110179211</td>\n",
       "      <td>D</td>\n",
       "      <td>democrat</td>\n",
       "      <td>speeches_111.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428339689</th>\n",
       "      <td>1110179211</td>\n",
       "      <td>D</td>\n",
       "      <td>democratic republican</td>\n",
       "      <td>speeches_111.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430264048 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             speech_id party_code             party_full       speech_file\n",
       "0            430000002          R             republican  speeches_043.txt\n",
       "78           430000002          R             republican  speeches_043.txt\n",
       "77           430000002          R             republican  speeches_043.txt\n",
       "76           430000002          R             republican  speeches_043.txt\n",
       "75           430000002          R             republican  speeches_043.txt\n",
       "...                ...        ...                    ...               ...\n",
       "1428339766  1110179211          D               democrat  speeches_111.txt\n",
       "1428339767  1110179211          D               democrat  speeches_111.txt\n",
       "1428339768  1110179211          D               democrat  speeches_111.txt\n",
       "1428339757  1110179211          D               democrat  speeches_111.txt\n",
       "1428339689  1110179211          D  democratic republican  speeches_111.txt\n",
       "\n",
       "[1430264048 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by speech_id\n",
    "df_party = df_speech_metadata\n",
    "df_party = df_party.sort_values(by='speech_id')\n",
    "df_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b306fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech_id</th>\n",
       "      <th>party_code</th>\n",
       "      <th>party_full_name</th>\n",
       "      <th>categorized_party</th>\n",
       "      <th>speech_file</th>\n",
       "      <th>file_num</th>\n",
       "      <th>year</th>\n",
       "      <th>filenum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430000002</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "      <td>43</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430000007</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "      <td>43</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>430000008</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "      <td>43</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430000010</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "      <td>43</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430000011</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_043.txt</td>\n",
       "      <td>43</td>\n",
       "      <td>1873</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297629</th>\n",
       "      <td>450129156</td>\n",
       "      <td>D</td>\n",
       "      <td>democratic republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_045.txt</td>\n",
       "      <td>45</td>\n",
       "      <td>1879</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297630</th>\n",
       "      <td>450129976</td>\n",
       "      <td>D</td>\n",
       "      <td>democratic republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_045.txt</td>\n",
       "      <td>45</td>\n",
       "      <td>1879</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297631</th>\n",
       "      <td>450131027</td>\n",
       "      <td>D</td>\n",
       "      <td>democratic republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_045.txt</td>\n",
       "      <td>45</td>\n",
       "      <td>1879</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297632</th>\n",
       "      <td>450133776</td>\n",
       "      <td>R</td>\n",
       "      <td>republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_045.txt</td>\n",
       "      <td>45</td>\n",
       "      <td>1879</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297633</th>\n",
       "      <td>450134253</td>\n",
       "      <td>D</td>\n",
       "      <td>democratic republican</td>\n",
       "      <td>Republican</td>\n",
       "      <td>speeches_045.txt</td>\n",
       "      <td>45</td>\n",
       "      <td>1879</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297634 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        speech_id party_code        party_full_name categorized_party  \\\n",
       "0       430000002          R             republican        Republican   \n",
       "1       430000007          R             republican        Republican   \n",
       "2       430000008          R             republican        Republican   \n",
       "3       430000010          R             republican        Republican   \n",
       "4       430000011          R             republican        Republican   \n",
       "...           ...        ...                    ...               ...   \n",
       "297629  450129156          D  democratic republican        Republican   \n",
       "297630  450129976          D  democratic republican        Republican   \n",
       "297631  450131027          D  democratic republican        Republican   \n",
       "297632  450133776          R             republican        Republican   \n",
       "297633  450134253          D  democratic republican        Republican   \n",
       "\n",
       "             speech_file  file_num  year  filenum  \n",
       "0       speeches_043.txt        43  1873       43  \n",
       "1       speeches_043.txt        43  1873       43  \n",
       "2       speeches_043.txt        43  1873       43  \n",
       "3       speeches_043.txt        43  1873       43  \n",
       "4       speeches_043.txt        43  1873       43  \n",
       "...                  ...       ...   ...      ...  \n",
       "297629  speeches_045.txt        45  1879       45  \n",
       "297630  speeches_045.txt        45  1879       45  \n",
       "297631  speeches_045.txt        45  1879       45  \n",
       "297632  speeches_045.txt        45  1879       45  \n",
       "297633  speeches_045.txt        45  1879       45  \n",
       "\n",
       "[297634 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_party['file_num'] = df_party['speech_file'].apply(lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "# merge the two dataframes based on speech_id and file_num\n",
    "df = pd.merge(df_party, df_year, left_on='speech_id', right_on='speech_id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ab1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# use conda environement \"spacy\" if the following command does not work\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# File paths\n",
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed_party/'\n",
    "party_full = './Congress/party_full.txt'\n",
    "first = 43\n",
    "last = 45\n",
    "encoding = 'Windows-1252'\n",
    "# Read and categorize party_full.txt\n",
    "dfparty = pd.read_csv(party_full, sep='|')\n",
    "\n",
    "# Function to categorize party based on 'party_full' content\n",
    "def categorize_party(party_full):\n",
    "    if 'republican' in party_full.lower().split(' ')[-1]:\n",
    "        return 'Republican'\n",
    "    elif 'democrat' in party_full.lower().split(' ')[-1]:\n",
    "        return 'Democrat'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the categorization function to the 'party_full' column\n",
    "dfparty['party_category'] = dfparty['party_full'].apply(categorize_party)\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "for congress in range(first, last+1):\n",
    "    speaker_map_file = os.path.join(indir, str(congress).zfill(3) + '_SpeakerMap.txt')\n",
    "    party_map = {}\n",
    "\n",
    "    # Parse SpeakerMap to get party affiliations\n",
    "    with open(speaker_map_file, encoding=encoding) as f:\n",
    "        for line in f.readlines()[1:]:  # Skip header line\n",
    "            parts = line.strip().split('|')\n",
    "            speech_id, party_code = parts[1], parts[7]\n",
    "            party_full_name = dfparty[dfparty['party'] == party_code]['party_full'].values[0] if len(dfparty[dfparty['party'] == party_code]) > 0 else 'Other'\n",
    "            party_map[speech_id] = categorize_party(party_full_name)\n",
    "\n",
    "    infile = os.path.join(indir, 'speeches_' + str(congress).zfill(3) + '.txt')\n",
    "    with open(infile, encoding=encoding) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Initialize party-specific outlines\n",
    "outlines_by_party = {'D': [], 'R': []}  # Initialize only for Democrat and Republican\n",
    "\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        line = line.strip()\n",
    "        parts = line.split('|')\n",
    "        line_id = parts[0]\n",
    "        \n",
    "        if line_id != 'speech_id':  # Skip header\n",
    "            party = party_map.get(line_id, 'Other')  # Default to 'Other' if not found\n",
    "            text = ' '.join(parts[1:])\n",
    "            parsed = nlp(text)\n",
    "\n",
    "            tokens = []\n",
    "            for sent in parsed.sents:\n",
    "                tokens.append([token.text for token in sent])\n",
    "\n",
    "            # Process text with spacy and collect features...\n",
    "            # Append to the correct party list\n",
    "            outlines_by_party[party].append({'id': line_id, 'tokens':tokens})\n",
    "\n",
    "    # Save outlines by party\n",
    "    for party, outlines in outlines_by_party.items():\n",
    "        outfile = os.path.join(outdir, f'speeches_{str(congress).zfill(3)}_{party[0]}.txt')\n",
    "        print(f\"Saving {len(outlines)} lines for party {party} to {outfile}\")\n",
    "        with open(outfile, 'w') as fo:\n",
    "            for line in outlines:\n",
    "                fo.write(json.dumps(line) + '\\n')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffef6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a test sentence. Please let me speak, or I will be forced to speak.\")\n",
    "\n",
    "sents = []\n",
    "tokens = []\n",
    "for sent in doc.sents:\n",
    "    sents.append(sent.text)\n",
    "    tokens.append([token.text for token in sent])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649b6247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'a', 'test', 'sentence', '.'],\n",
       " ['Please',\n",
       "  'let',\n",
       "  'me',\n",
       "  'speak',\n",
       "  ',',\n",
       "  'or',\n",
       "  'I',\n",
       "  'will',\n",
       "  'be',\n",
       "  'forced',\n",
       "  'to',\n",
       "  'speak',\n",
       "  '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38156464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.2)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.3.0\n",
      "    Uninstalling en-core-web-sm-3.3.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.3.0\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f53492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: spacy 3.7.4\n",
      "Uninstalling spacy-3.7.4:\n",
      "  Successfully uninstalled spacy-3.7.4\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n",
      "/home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall spacy -y\n",
    "!pip install spacy -y\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac39a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a93e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957e645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490aaed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming spacy is already loaded and nlp model is available\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed_party/'\n",
    "\n",
    "first = 110\n",
    "last = 111\n",
    "encoding = 'Windows-1252'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "for congress in range(first, last+1):\n",
    "    speaker_map_file = os.path.join(indir, str(congress).zfill(3) + '_SpeakerMap.txt')\n",
    "    party_map = {}\n",
    "\n",
    "    # Parse SpeakerMap to get party affiliations\n",
    "    with open(speaker_map_file, encoding=encoding) as f:\n",
    "        for line in f.readlines()[1:]:  # Skip header line\n",
    "            parts = line.strip().split('|')\n",
    "            speech_id, party = parts[1], parts[7]\n",
    "            party_map[speech_id] = party\n",
    "\n",
    "    infile = os.path.join(indir, 'speeches_' + str(congress).zfill(3) + '.txt')\n",
    "    with open(infile, encoding=encoding) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Initialize party-specific outlines\n",
    "    outlines_by_party = {'R': [], 'D': [], 'O': []}  # R: Republican, D: Democratic, O: Other\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        line = line.strip()\n",
    "        parts = line.split('|')\n",
    "        line_id = parts[0]\n",
    "        \n",
    "        if line_id != 'speech_id':  # Skip header\n",
    "            party = party_map.get(line_id, 'O')  # Default to 'Other' if not found\n",
    "            text = ' '.join(parts[1:])\n",
    "            parsed = nlp(text)\n",
    "\n",
    "            # Process text with spacy and collect features...\n",
    "\n",
    "            # Append to the correct party list\n",
    "            outlines_by_party[party[0] if party in ['R', 'D'] else 'O'].append({'id': line_id, ...})\n",
    "\n",
    "    # Save outlines by party\n",
    "    for party, outlines in outlines_by_party.items():\n",
    "        outfile = os.path.join(outdir, f'speeches_{str(congress).zfill(3)}_{party}.txt')\n",
    "        print(f\"Saving {len(outlines)} lines for party {party} to {outfile}\")\n",
    "        with open(outfile, 'w') as fo:\n",
    "            for line in outlines:\n",
    "                fo.write(json.dumps(line) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e1e59-0256-44e6-be68-2b4aaa54e4c2",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "# Print the number of arguments passed to the script\n",
    "print(f'Number of arguments: {len(sys.argv)}')\n",
    "\n",
    "# Print the arguments passed to the script\n",
    "print('Arguments:')\n",
    "for i, arg in enumerate(sys.argv):\n",
    "    print(f'{i}: {arg}')\n",
    "\n",
    "    \n",
    "sys.argv[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e1eaf-2195-4979-9e01-7b7c1011e15c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## in the bash: notes the i should be converted to int(i) in the script.py\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# Iterate over the numbers from 40 to 100\n",
    "for i in {40..100}\n",
    "do\n",
    "    # Run the 'nohup' command with the current number as an argument in the background\n",
    "    nohup python script.py $i &\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095946f",
   "metadata": {},
   "source": [
    "# congress_parse_speeches_year_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f65d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# File paths and settings\n",
    "mainpath = '/home/local/PSYCH-ADS/xuqian_chen/YES_lab/Amber/nlp'\n",
    "indir = mainpath+'/Congress/hein-bound/'\n",
    "# outdir = mainpath+'/Congress/hein-bound_parsed_party_year/'\n",
    "outdir = './'\n",
    "party_full = mainpath+'/Congress/party_full.txt'\n",
    "first = 43\n",
    "last = 44\n",
    "encoding = 'Windows-1252'\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Initialize spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv(mainpath+'/Congress/congress_party_year.csv')\n",
    "\n",
    "# Only keep rows with party_code 'D' or 'R'\n",
    "df = df[df['party_code'].isin(['D', 'R'])]\n",
    "\n",
    "# Function to process each speech in a given congress session\n",
    "def process_speeches(group):\n",
    "    outlines_by_party = {'D': [], 'R': []}  # Initialize only for Democrat and Republican\n",
    "    for _, row in group.iterrows():\n",
    "        speech_file_path = os.path.join(indir, row['speech_file'])\n",
    "        with open(speech_file_path, encoding=encoding) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        party_map = {row['speech_id']: row['party_code'] for _, row in group.iterrows()}\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            parts = line.split('|')\n",
    "            line_id = parts[0]\n",
    "            \n",
    "            if line_id != 'speech_id':  # Skip header\n",
    "                party = party_map.get(line_id)\n",
    "                if party in outlines_by_party:  # Check if party is either 'D' or 'R'\n",
    "                    text = ' '.join(parts[1:])\n",
    "                    parsed = nlp(text)\n",
    "                    tokens = [token.text for sent in parsed.sents for token in sent]\n",
    "                    outlines_by_party[party].append({'id': line_id, 'tokens': tokens})\n",
    "    \n",
    "    return outlines_by_party\n",
    "\n",
    "# Get the number of available CPU cores and leave one unoccupied\n",
    "import multiprocessing\n",
    "num_cores = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "# Use ProcessPoolExecutor to parallelize the processing\n",
    "with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "    future_to_congress = {executor.submit(process_speeches, group): congress for congress, group in df.groupby('filenum')}\n",
    "    \n",
    "    for future in as_completed(future_to_congress):\n",
    "        congress = future_to_congress[future]\n",
    "        outlines_by_party = future.result()\n",
    "        \n",
    "        for party, outlines in outlines_by_party.items():\n",
    "            # save just the tokens, no speech id\n",
    "            outfile = os.path.join(outdir, f'{congress}_{party}.txt')\n",
    "            with open(outfile, 'w') as f:\n",
    "                for outline in outlines:\n",
    "                    f.write(' '.join(outline['tokens']) + '\\n')\n",
    "\n",
    "            \n",
    "            print(f\"Saved {len(outlines)} lines for party {party} to {outfile}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590cc91",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b6f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# File paths and settings\n",
    "mainpath = '/home/local/PSYCH-ADS/xuqian_chen/YES_lab/Amber/nlp'\n",
    "indir = mainpath+'/Congress/hein-bound/'\n",
    "# outdir = mainpath+'/Congress/hein-bound_parsed_party_year/'\n",
    "outdir = './'\n",
    "party_full = mainpath+'/Congress/party_full.txt'\n",
    "first = 43\n",
    "last = 44\n",
    "encoding = 'Windows-1252'\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Initialize spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "df = pd.read_csv(mainpath+'/Congress/congress_party_year.csv')\n",
    "\n",
    "# Only keep rows with party_code 'D' or 'R'\n",
    "df = df[df['party_code'].isin(['D', 'R'])]\n",
    "\n",
    "# Function to process each speech in a given congress session\n",
    "def process_speeches(group):\n",
    "    outlines_by_party = {'D': [], 'R': []}  # Initialize only for Democrat and Republican\n",
    "    for _, row in group.iterrows():\n",
    "        speech_file_path = os.path.join(indir, row['speech_file'])\n",
    "        with open(speech_file_path, encoding=encoding) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        party_map = {row['speech_id']: row['party_code'] for _, row in group.iterrows()}\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            parts = line.split('|')\n",
    "            line_id = parts[0]\n",
    "            \n",
    "            if line_id != 'speech_id':  # Skip header\n",
    "                party = party_map.get(line_id)\n",
    "                if party in outlines_by_party:  # Check if party is either 'D' or 'R'\n",
    "                    text = ' '.join(parts[1:])\n",
    "                    parsed = nlp(text)\n",
    "                    tokens = [token.text for sent in parsed.sents for token in sent]\n",
    "                    outlines_by_party[party].append({'id': line_id, 'tokens': tokens})\n",
    "    \n",
    "    return outlines_by_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a976287",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     11\u001b[0m     speech_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(indir, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech_file\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspeech_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     13\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     15\u001b[0m     party_map \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparty_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39miterrows()}\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/codecs.py:260\u001b[0m, in \u001b[0;36mIncrementalDecoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalDecoder\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    An IncrementalDecoder decodes an input in multiple steps. The input can\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    be passed piece by piece to the decode() method. The IncrementalDecoder\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    remembers the state of the decoding process between calls to decode().\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m        Create an IncrementalDecoder instance.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m        for a list of possible values.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors \u001b[38;5;241m=\u001b[39m errors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Sort by 'filenum' and get the first two unique file numbers\n",
    "unique_filenum = df['filenum'].unique()\n",
    "if len(unique_filenum) > 2:\n",
    "    unique_filenum = unique_filenum[:2]  # Select only the first two for testing\n",
    "df = df[df['filenum'].isin(unique_filenum)]\n",
    "\n",
    "# Sequential processing\n",
    "for congress, group in df.groupby('filenum'):\n",
    "    outlines_by_party = {'D': [], 'R': []}  # Initialize only for Democrat and Republican\n",
    "    for _, row in group.iterrows():\n",
    "        speech_file_path = os.path.join(indir, row['speech_file'])\n",
    "        with open(speech_file_path, encoding=encoding) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        party_map = {row['speech_id']: row['party_code'] for _, row in group.iterrows()}\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            parts = line.split('|')\n",
    "            line_id = parts[0]\n",
    "            \n",
    "            if line_id != 'speech_id':  # Skip header\n",
    "                party = party_map.get(line_id)\n",
    "                if party in outlines_by_party:  # Check if party is either 'D' or 'R'\n",
    "                    text = ' '.join(parts[1:])\n",
    "                    parsed = nlp(text)\n",
    "                    tokens = [token.text for sent in parsed.sents for token in sent]\n",
    "                    outlines_by_party[party].append({'id': line_id, 'tokens': tokens})\n",
    "    \n",
    "    for party, outlines in outlines_by_party.items():\n",
    "        outfile = os.path.join(outdir, f'{congress}_{party}.txt')\n",
    "        with open(outfile, 'w') as f:\n",
    "            for outline in outlines:\n",
    "                f.write(' '.join(outline['tokens']) + '\\n')\n",
    "        print(f\"Saved {len(outlines)} lines for party {party} to {outfile}\")\n",
    "\n",
    "print(\"Processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0243713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party_map.get(430000046)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4c5ba-b214-4588-9781-007a60ebf794",
   "metadata": {},
   "source": [
    "# tokenize_hein_bound.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6872d64b-9509-43f9-9d96-3f209fdbada9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy\n",
      "./Congress/hein-bound/speeches_043.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 119303/119303 [19:17<00:00, 103.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 119302 lines to ./Congress/hein-bound_parsed/speeches_043.jsonlist\n",
      "./Congress/hein-bound/speeches_044.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████▉                                                                                        | 8425/114781 [01:43<21:46, 81.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m18940614\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     60\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(parts[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 61\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     sents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/language.py:1005\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    986\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    989\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    990\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1007\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/language.py:1095\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1095\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE866\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/language.py:1088\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1086\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/tokenizer.pyx:155\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/tokenizer.pyx:191\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/tokenizer.pyx:395\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/tokenizer.pyx:473\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/vocab.pyx:160\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/vocab.pyx:197\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/spacy/lang/lex_attrs.py:145\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    141\u001b[0m             shape\u001b[38;5;241m.\u001b[39mappend(shape_char)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(shape)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m string\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprefix\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "from glob import glob\n",
    "from optparse import OptionParser\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "# usage = \"%prog hein-bound-dir output-dir\"\n",
    "# parser = OptionParser(usage=usage)\n",
    "# parser.add_option('--first', type=int, default=43,\n",
    "#                   help='First congress: default=%default')\n",
    "# parser.add_option('--last', type=int, default=111,\n",
    "#                   help='Last congress: default=%default')\n",
    "\n",
    "# (options, args) = parser.parse_args()\n",
    "\n",
    "# indir = args[0]\n",
    "# outdir = args[1]\n",
    "indir = './Congress/hein-bound/'\n",
    "outdir = './Congress/hein-bound_parsed/'\n",
    "\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "first = 43\n",
    "last = 44\n",
    "\n",
    "print(\"Loading spacy\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for congress in range(first, last+1):\n",
    "    infile = os.path.join(indir, 'speeches_' + str(congress).zfill(3) + '.txt')\n",
    "    descr_file = os.path.join(indir, 'descr_' + str(congress).zfill(3) + '.txt')\n",
    "    print(infile)\n",
    "    basename = os.path.splitext(os.path.basename(infile))[0]\n",
    "    outlines = []\n",
    "    speech_dates = {}\n",
    "    with open(descr_file, encoding='Windows-1252') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.split('|')\n",
    "        speech_id = parts[0]\n",
    "        date = parts[2]\n",
    "        speech_dates[speech_id] = date\n",
    "\n",
    "    with open(infile, encoding='Windows-1252') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in tqdm.tqdm(lines):\n",
    "        line = line.strip()\n",
    "        parts = line.split('|')\n",
    "        line_id = parts[0]\n",
    "        # drop the header\n",
    "        if line_id != 'speech_id':\n",
    "            date = speech_dates[line_id]\n",
    "            # skip one day that has is corrupted by data from 1994\n",
    "            if date != '18940614':\n",
    "                text = ' '.join(parts[1:])\n",
    "                parsed = nlp(text)\n",
    "                sents = []\n",
    "                tokens = []\n",
    "                for sent in parsed.sents:\n",
    "                    sents.append(sent.text)\n",
    "                    tokens.append([token.text for token in sent])\n",
    "\n",
    "                assert len(sents) == len(tokens)\n",
    "\n",
    "                rejoined_sents = []\n",
    "                rejoined_tokens = []\n",
    "                if len(sents) > 0:\n",
    "                    current_sent = sents[0]\n",
    "                    current_tokens = tokens[0]\n",
    "                    if len(sents) > 1:\n",
    "                        for sent_i in range(1, len(sents)):\n",
    "                            # look to see if this might be a false sentence break\n",
    "                            if sents[sent_i-1][-1] == '.' and (sents[sent_i][0].islower() or sents[sent_i][0].isdigit() or sents[sent_i][0] == '$' or sents[sent_i][0] == '%'):\n",
    "                                # if so, extend the previous sentence / tokens\n",
    "                                current_sent += ' ' + sents[sent_i]\n",
    "                                current_tokens.extend(tokens[sent_i])\n",
    "                            else:\n",
    "                                # otherwise, add the previous to the list, and start a new one\n",
    "                                rejoined_sents.append(current_sent)\n",
    "                                rejoined_tokens.append(current_tokens)\n",
    "                                current_sent = sents[sent_i]\n",
    "                                current_tokens = tokens[sent_i]\n",
    "                    # add the current to the list\n",
    "                    rejoined_sents.append(current_sent)\n",
    "                    rejoined_tokens.append(current_tokens)\n",
    "\n",
    "                outlines.append({'infile': basename, 'id': line_id, 'sents': rejoined_sents, 'tokens': rejoined_tokens})\n",
    "\n",
    "    outfile = os.path.join(outdir, basename + '.jsonlist')\n",
    "    print(\"Saving {:d} lines to {:s}\".format(len(outlines), outfile))\n",
    "    with open(outfile, 'w') as fo:\n",
    "        for line in outlines:\n",
    "            fo.write(json.dumps(line) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3a6e5-01f9-4665-ae19-0ba7d1cd6018",
   "metadata": {},
   "source": [
    "# rejoin_into_pieces_by_congress.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1096a6e5-5b9f-4811-82d9-433cb0128f58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py outdir\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadOptionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/optparse.py:1387\u001b[0m, in \u001b[0;36mOptionParser.parse_args\u001b[0;34m(self, args, values)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1387\u001b[0m     stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BadOptionError, OptionValueError) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/optparse.py:1431\u001b[0m, in \u001b[0;36mOptionParser._process_args\u001b[0;34m(self, largs, rargs, values)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# process a cluster of short options (possibly with\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;66;03m# value(s) for the last one only)\u001b[39;00m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_short_opts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_interspersed_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/optparse.py:1513\u001b[0m, in \u001b[0;36mOptionParser._process_short_opts\u001b[0;34m(self, rargs, values)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m option:\n\u001b[0;32m-> 1513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadOptionError(opt)\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m option\u001b[38;5;241m.\u001b[39mtakes_value():\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;66;03m# Any characters left in arg?  Pretend they're the\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;66;03m# next arg, and stop consuming characters of arg.\u001b[39;00m\n",
      "\u001b[0;31mBadOptionError\u001b[0m: no such option: -f",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 144>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--use-sents\u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     27\u001b[0m                   help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse sentences rather than tokens (avoid excess spaces): default=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mefault\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m (options, args) \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m outdir \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/optparse.py:1389\u001b[0m, in \u001b[0;36mOptionParser.parse_args\u001b[0;34m(self, args, values)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BadOptionError, OptionValueError) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m args \u001b[38;5;241m=\u001b[39m largs \u001b[38;5;241m+\u001b[39m rargs\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/optparse.py:1569\u001b[0m, in \u001b[0;36mOptionParser.error\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 1569\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prog_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/optparse.py:1559\u001b[0m, in \u001b[0;36mOptionParser.exit\u001b[0;34m(self, status, msg)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg)\n\u001b[0;32m-> 1559\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/interactiveshell.py:1972\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   1970\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1971\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1972\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1976\u001b[0m         \u001b[38;5;66;03m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[1;32m   1978\u001b[0m         \u001b[38;5;66;03m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:585\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:443\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    440\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    441\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    442\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 443\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:1118\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m tb\n\u001b[0;32m-> 1118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:1012\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1009\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:865\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    858\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    863\u001b[0m ):\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m    869\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:799\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    797\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(etype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m    798\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    802\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    803\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/IPython/core/ultratb.py:854\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m    848\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    849\u001b[0m options \u001b[38;5;241m=\u001b[39m stack_data\u001b[38;5;241m.\u001b[39mOptions(\n\u001b[1;32m    850\u001b[0m     before\u001b[38;5;241m=\u001b[39mbefore,\n\u001b[1;32m    851\u001b[0m     after\u001b[38;5;241m=\u001b[39mafter,\n\u001b[1;32m    852\u001b[0m     pygments_formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[1;32m    853\u001b[0m )\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[tb_offset:]\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/stack_data/core.py:546\u001b[0m, in \u001b[0;36mFrameInfo.stack_data\u001b[0;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack_data\u001b[39m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m         collapse_repeated_frames: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrameInfo\u001b[39m\u001b[38;5;124m'\u001b[39m, RepeatedFrames]]:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    An iterator of FrameInfo and RepeatedFrames objects representing\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    and optionally an Options object to configure.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# Reverse the stack from a frame so that it's in the same order\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;66;03m# as the order from a traceback, which is the order of a printed\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# traceback when read top to bottom (most recent call last)\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_frame(frame_or_tb):\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/stack_data/utils.py:98\u001b[0m, in \u001b[0;36miter_stack\u001b[0;34m(frame_or_tb)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame_or_tb:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m frame_or_tb\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     99\u001b[0m         frame_or_tb \u001b[38;5;241m=\u001b[39m frame_or_tb\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/stack_data/utils.py:91\u001b[0m, in \u001b[0;36mis_frame\u001b[0;34m(frame_or_tb)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_frame\u001b[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[43massert_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracebackType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame_or_tb, (types\u001b[38;5;241m.\u001b[39mFrameType,))\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/stack_data/utils.py:172\u001b[0m, in \u001b[0;36massert_\u001b[0;34m(condition, error)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    171\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAssertionError\u001b[39;00m(error)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from optparse import OptionParser\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Same as rejoin_into_pieces, except output one file per congress\n",
    "\n",
    "def main():\n",
    "    usage = \"%prog outdir\"\n",
    "    parser = OptionParser(usage=usage)\n",
    "    parser.add_option('--hein-bound-dir', type=str, default='data/speeches/Congress/hein-bound-tokenized-rejoined',\n",
    "                      help='Issue: default=%default')\n",
    "    parser.add_option('--hein-daily-dir', type=str, default='data/speeches/Congress/hein-daily-tokenized',\n",
    "                      help='Issue: default=%default')\n",
    "    parser.add_option('--first', type=int, default=43,\n",
    "                      help='First congress: default=%default')\n",
    "    parser.add_option('--last', type=int, default=114,\n",
    "                      help='Last congress: default=%default')\n",
    "    parser.add_option('--max', type=int, default=375,\n",
    "                      help='Max tokens per block: default=%default')\n",
    "    parser.add_option('--keep-boundaries', action=\"store_true\", default=False,\n",
    "                      help='Output each sentence on a separate line: default=%default')\n",
    "    parser.add_option('--replace-periods', action=\"store_true\", default=False,\n",
    "                      help='Change periods that look wrong to commas: default=%default')\n",
    "    parser.add_option('--use-sents', action=\"store_true\", default=False,\n",
    "                      help='Use sentences rather than tokens (avoid excess spaces): default=%default')\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "\n",
    "    outdir = args[0]\n",
    "\n",
    "    hein_bound_dir = options.hein_bound_dir\n",
    "    hein_daily_dir = options.hein_daily_dir\n",
    "    first = options.first\n",
    "    last = options.last\n",
    "    max_length = options.max\n",
    "    keep_boundaries = options.keep_boundaries\n",
    "    replace_periods = options.replace_periods\n",
    "    use_sents = options.use_sents\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    for congress in range(first, last+1):\n",
    "        if congress < 100:\n",
    "            infile = os.path.join(hein_bound_dir, 'speeches_0' + str(congress) + '.jsonlist')\n",
    "        elif congress > 111:\n",
    "            infile = os.path.join(hein_daily_dir, 'speeches_' + str(congress) + '.jsonlist')\n",
    "        else:\n",
    "            infile = os.path.join(hein_bound_dir, 'speeches_' + str(congress) + '.jsonlist')\n",
    "\n",
    "        print(infile)\n",
    "        with open(infile) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        lengths = []\n",
    "        outlines = []\n",
    "        # read each speech, one by one\n",
    "        for line in lines:\n",
    "            line = json.loads(line)\n",
    "            speech_id = line['id']  # speehch id\n",
    "            if use_sents:\n",
    "                sents = line['sents']  # list of strings\n",
    "            else:\n",
    "                sents = line['tokens']  # list of (list of strings)\n",
    "\n",
    "            joined_sents = []\n",
    "            n_tokens = 0\n",
    "            n_chars = 0\n",
    "            # first try to rejoin bad splits\n",
    "            for tokens in sents:\n",
    "                if use_sents:\n",
    "                    # put the string in a list to conform to below\n",
    "                    n_chars += len(tokens)\n",
    "                    n_tokens += len(tokens.split())\n",
    "                    tokens = [tokens]\n",
    "                else:\n",
    "                    n_chars += sum([len(t) for t in tokens]) + len(tokens) - 1\n",
    "                    n_tokens += len(tokens)\n",
    "                # if this is the first sentence, start a new string\n",
    "                if len(joined_sents) == 0:\n",
    "                    joined_sents.append(' '.join(tokens))\n",
    "                # if it looks like this is not the start of a sentence:\n",
    "                elif tokens[0][0].islower() or tokens[0][0].isdigit() or tokens[0][0] == '$' or tokens[0][0] == '%':\n",
    "                    # change a period to a comma at the end of the last group, if it is there\n",
    "                    if replace_periods and joined_sents[-1][-1] == '.':\n",
    "                        joined_sents[-1] = joined_sents[-1][:-1] + ', ' + ' '.join(tokens)\n",
    "                    else:\n",
    "                        joined_sents[-1] += ' ' + ' '.join(tokens)\n",
    "                    n_chars += 1\n",
    "                # otherwise, start a new string\n",
    "                else:\n",
    "                    joined_sents.append(' '.join(tokens))\n",
    "\n",
    "            if keep_boundaries:\n",
    "                check = 0\n",
    "                for s_i, sent in enumerate(joined_sents):\n",
    "                    lengths.append(len(sent))\n",
    "                    check += len(sent)\n",
    "                    outlines.append({'id': speech_id + '_s' + f'{s_i:03}', 'text': sent})\n",
    "\n",
    "                assert check == n_chars\n",
    "\n",
    "            else:\n",
    "                # then connect these sentence into blocks of up to max_length tokens or keep as sentences:\n",
    "                cur_length = 0\n",
    "                output_blocks = []\n",
    "                for sent in joined_sents:\n",
    "                    sent_len = len(sent.split())\n",
    "                    if len(output_blocks) == 0:\n",
    "                        output_blocks.append(sent)\n",
    "                        cur_length += sent_len\n",
    "                    elif cur_length + sent_len > max_length:\n",
    "                        # append this as a new block and reset count\n",
    "                        output_blocks.append(sent)\n",
    "                        cur_length = sent_len\n",
    "                    else:\n",
    "                        output_blocks[-1] += ' ' + sent\n",
    "                        cur_length += sent_len\n",
    "\n",
    "                check = 0\n",
    "                for block_i, block in enumerate(output_blocks):\n",
    "                    block_length = len(block.split())\n",
    "                    check += block_length\n",
    "                    lengths.append(block_length)\n",
    "                    outlines.append({'id': speech_id + '_b' + f'{block_i:03}', 'text': block})\n",
    "\n",
    "                try:\n",
    "                    assert check == n_tokens\n",
    "                except AssertionError as e:\n",
    "                    print(n_tokens, check, len(outlines))\n",
    "\n",
    "        print(np.mean(lengths), np.median(lengths), np.max(lengths), sum([1 for length in lengths if length > max_length]) / len(lengths))\n",
    "        longest = int(np.argmax(lengths))\n",
    "        print(outlines[longest])\n",
    "\n",
    "        outfile = os.path.join(outdir, 'segments-' + str(congress).zfill(3) + '.jsonlist')\n",
    "        with open(outfile, 'w') as f:\n",
    "            for line in outlines:\n",
    "                f.write(json.dumps(line) + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d165b1e-721a-4e3e-998b-3c7951a9c404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"infile\": \"speeches_043\", \"id\": \"430000001\", \"sents\": [\"The Secretary will read the names of the newlyelected Senators.\", \"The list was read as follows: lion.\", \"Bainbridge Wadleigh. of New Hampshire.\", \"Hon.\", \"Justin S. Morrill. of Vermont.\", \"Hon.\", \"Orris S. Ferry. of Connecticut.\", \"Hon.\", \"Roscoe Coukling. of New York.\", \"Hon.\", \"Simon Cameron. of Pennsylvania. lan.\", \"George h. Dennis. of Maryland.\", \"Hon.\", \"Augustus S. Merrimon. of North Carolina.\", \"Hon.\", \"John J. Patterson. of South Carolina.\", \"Hon.\", \"Simon B. Conover. of Florida.\", \"Ion.\", \"George E. Spencer. of Alabama.\", \"H10a.\", \"Stephen WV.\", \"Dorsey. of Arkansas. lion.\", \"John B. Gordon. of Georgia.\", \"Hon.\", \"Lewis V. Bogy. of Missouri.\", \"Hon.\", \"Thomas C. MeCreery. of Kentucky.\", \"Hon.\", \"John Sherman. of Ohio.\", \"Hon. Oliver P1.\", \"Morton. of Indiana.\", \"Hon.\", \"Richard 3.\", \"Oglesby. of Illinois.\", \"Hon.\", \"Timothy 0.\", \"Howe. of Wisconsin.\", \"Hon.\", \"William B. Allison. of Iowa.\", \"Hon.\", \"John J. Ingalls. of Kansas.\", \"Hon.\", \"Aaron A. Sargent. of California.\", \"Hon.\", \"John Il.\", \"Mitchell of Oregon.\", \"Hon.\", \"John P. Jones. otNevada.\", \"When the name of Mr. Conkling was called.\"], \"tokens\": [[\"The\", \"Secretary\", \"will\", \"read\", \"the\", \"names\", \"of\", \"the\", \"newlyelected\", \"Senators\", \".\"], [\"The\", \"list\", \"was\", \"read\", \"as\", \"follows\", \":\", \"lion\", \".\"], [\"Bainbridge\", \"Wadleigh\", \".\", \"of\", \"New\", \"Hampshire\", \".\"], [\"Hon\", \".\"], [\"Justin\", \"S.\", \"Morrill\", \".\", \"of\", \"Vermont\", \".\"], [\"Hon\", \".\"], [\"Orris\", \"S.\", \"Ferry\", \".\", \"of\", \"Connecticut\", \".\"], [\"Hon\", \".\"], [\"Roscoe\", \"Coukling\", \".\", \"of\", \"New\", \"York\", \".\"], [\"Hon\", \".\"], [\"Simon\", \"Cameron\", \".\", \"of\", \"Pennsylvania\", \".\", \"lan\", \".\"], [\"George\", \"h.\", \"Dennis\", \".\", \"of\", \"Maryland\", \".\"], [\"Hon\", \".\"], [\"Augustus\", \"S.\", \"Merrimon\", \".\", \"of\", \"North\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Patterson\", \".\", \"of\", \"South\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"Simon\", \"B.\", \"Conover\", \".\", \"of\", \"Florida\", \".\"], [\"Ion\", \".\"], [\"George\", \"E.\", \"Spencer\", \".\", \"of\", \"Alabama\", \".\"], [\"H10a\", \".\"], [\"Stephen\", \"WV\", \".\"], [\"Dorsey\", \".\", \"of\", \"Arkansas\", \".\", \"lion\", \".\"], [\"John\", \"B.\", \"Gordon\", \".\", \"of\", \"Georgia\", \".\"], [\"Hon\", \".\"], [\"Lewis\", \"V.\", \"Bogy\", \".\", \"of\", \"Missouri\", \".\"], [\"Hon\", \".\"], [\"Thomas\", \"C.\", \"MeCreery\", \".\", \"of\", \"Kentucky\", \".\"], [\"Hon\", \".\"], [\"John\", \"Sherman\", \".\", \"of\", \"Ohio\", \".\"], [\"Hon\", \".\", \"Oliver\", \"P1\", \".\"], [\"Morton\", \".\", \"of\", \"Indiana\", \".\"], [\"Hon\", \".\"], [\"Richard\", \"3\", \".\"], [\"Oglesby\", \".\", \"of\", \"Illinois\", \".\"], [\"Hon\", \".\"], [\"Timothy\", \"0\", \".\"], [\"Howe\", \".\", \"of\", \"Wisconsin\", \".\"], [\"Hon\", \".\"], [\"William\", \"B.\", \"Allison\", \".\", \"of\", \"Iowa\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Ingalls\", \".\", \"of\", \"Kansas\", \".\"], [\"Hon\", \".\"], [\"Aaron\", \"A.\", \"Sargent\", \".\", \"of\", \"California\", \".\"], [\"Hon\", \".\"], [\"John\", \"Il\", \".\"], [\"Mitchell\", \"of\", \"Oregon\", \".\"], [\"Hon\", \".\"], [\"John\", \"P.\", \"Jones\", \".\", \"otNevada\", \".\"], [\"When\", \"the\", \"name\", \"of\", \"Mr.\", \"Conkling\", \"was\", \"called\", \".\"]]}\\n',\n",
       " '{\"infile\": \"speeches_043\", \"id\": \"430000002\", \"sents\": [\"said: Mr. President. owing to some inadvertence the credentials of the Senatorelect from Now York have not been presented in this body.\", \"It is a matter of public notoriety that he has been elected. and. in accordance with the usage of the body.\", \"I move that the oath of office be administered to him.\"], \"tokens\": [[\"said\", \":\", \"Mr.\", \"President\", \".\", \"owing\", \"to\", \"some\", \"inadvertence\", \"the\", \"credentials\", \"of\", \"the\", \"Senatorelect\", \"from\", \"Now\", \"York\", \"have\", \"not\", \"been\", \"presented\", \"in\", \"this\", \"body\", \".\"], [\"It\", \"is\", \"a\", \"matter\", \"of\", \"public\", \"notoriety\", \"that\", \"he\", \"has\", \"been\", \"elected\", \".\", \"and\", \".\", \"in\", \"accordance\", \"with\", \"the\", \"usage\", \"of\", \"the\", \"body\", \".\"], [\"I\", \"move\", \"that\", \"the\", \"oath\", \"of\", \"office\", \"be\", \"administered\", \"to\", \"him\", \".\"]]}\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = './Congress/hein-bound_parsed/speeches_043.jsonlist'\n",
    "with open(infile) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2d462b-39aa-4151-9281-70da616dc17f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"id\": \"430000001\", \"tokens\": [[\"The\", \"Secretary\", \"will\", \"read\", \"the\", \"names\", \"of\", \"the\", \"newlyelected\", \"Senators\", \".\"], [\"The\", \"list\", \"was\", \"read\", \"as\", \"follows\", \":\", \"lion\", \".\"], [\"Bainbridge\", \"Wadleigh\", \".\", \"of\", \"New\", \"Hampshire\", \".\"], [\"Hon\", \".\"], [\"Justin\", \"S.\", \"Morrill\", \".\", \"of\", \"Vermont\", \".\"], [\"Hon\", \".\"], [\"Orris\", \"S.\", \"Ferry\", \".\", \"of\", \"Connecticut\", \".\"], [\"Hon\", \".\"], [\"Roscoe\", \"Coukling\", \".\", \"of\", \"New\", \"York\", \".\"], [\"Hon\", \".\"], [\"Simon\", \"Cameron\", \".\", \"of\", \"Pennsylvania\", \".\"], [\"lan\", \".\"], [\"George\", \"h.\", \"Dennis\", \".\"], [\"of\", \"Maryland\", \".\"], [\"Hon\", \".\"], [\"Augustus\", \"S.\", \"Merrimon\", \".\", \"of\", \"North\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Patterson\", \".\", \"of\", \"South\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"Simon\", \"B.\", \"Conover\", \".\", \"of\", \"Florida\", \".\"], [\"Ion\", \".\"], [\"George\", \"E.\", \"Spencer\", \".\", \"of\", \"Alabama\", \".\"], [\"H10a\", \".\"], [\"Stephen\", \"WV\", \".\"], [\"Dorsey\", \".\", \"of\", \"Arkansas\", \".\"], [\"lion\", \".\"], [\"John\", \"B.\", \"Gordon\", \".\", \"of\", \"Georgia\", \".\"], [\"Hon\", \".\"], [\"Lewis\", \"V.\", \"Bogy\", \".\", \"of\", \"Missouri\", \".\"], [\"Hon\", \".\"], [\"Thomas\", \"C.\", \"MeCreery\", \".\", \"of\", \"Kentucky\", \".\"], [\"Hon\", \".\"], [\"John\", \"Sherman\", \".\", \"of\", \"Ohio\", \".\"], [\"Hon\", \".\", \"Oliver\", \"P1\", \".\"], [\"Morton\", \".\", \"of\", \"Indiana\", \".\"], [\"Hon\", \".\"], [\"Richard\", \"3\", \".\"], [\"Oglesby\", \".\", \"of\", \"Illinois\", \".\"], [\"Hon\", \".\"], [\"Timothy\", \"0\", \".\"], [\"Howe\", \".\", \"of\", \"Wisconsin\", \".\"], [\"Hon\", \".\"], [\"William\", \"B.\", \"Allison\", \".\"], [\"of\", \"Iowa\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Ingalls\", \".\", \"of\", \"Kansas\", \".\"], [\"Hon\", \".\"], [\"Aaron\", \"A.\", \"Sargent\", \".\", \"of\", \"California\", \".\"], [\"Hon\", \".\"], [\"John\", \"Il\", \".\"], [\"Mitchell\", \"of\", \"Oregon\", \".\"], [\"Hon\", \".\"], [\"John\", \"P.\", \"Jones\", \".\"], [\"otNevada\", \".\"], [\"When\", \"the\", \"name\", \"of\", \"Mr.\", \"Conkling\", \"was\", \"called\", \".\"]], \"lemmas\": [[\"the\", \"Secretary\", \"will\", \"read\", \"the\", \"name\", \"of\", \"the\", \"newlyelecte\", \"senator\", \".\"], [\"the\", \"list\", \"be\", \"read\", \"as\", \"follow\", \":\", \"lion\", \".\"], [\"Bainbridge\", \"Wadleigh\", \".\", \"of\", \"New\", \"Hampshire\", \".\"], [\"hon\", \".\"], [\"Justin\", \"S.\", \"Morrill\", \".\", \"of\", \"Vermont\", \".\"], [\"hon\", \".\"], [\"Orris\", \"S.\", \"Ferry\", \".\", \"of\", \"Connecticut\", \".\"], [\"hon\", \".\"], [\"Roscoe\", \"Coukling\", \".\", \"of\", \"New\", \"York\", \".\"], [\"hon\", \".\"], [\"Simon\", \"Cameron\", \".\", \"of\", \"Pennsylvania\", \".\"], [\"lan\", \".\"], [\"George\", \"h.\", \"Dennis\", \".\"], [\"of\", \"Maryland\", \".\"], [\"hon\", \".\"], [\"Augustus\", \"S.\", \"Merrimon\", \".\", \"of\", \"North\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Patterson\", \".\", \"of\", \"South\", \"Carolina\", \".\"], [\"hon\", \".\"], [\"Simon\", \"B.\", \"Conover\", \".\", \"of\", \"Florida\", \".\"], [\"Ion\", \".\"], [\"George\", \"E.\", \"Spencer\", \".\", \"of\", \"Alabama\", \".\"], [\"H10a\", \".\"], [\"Stephen\", \"WV\", \".\"], [\"Dorsey\", \".\", \"of\", \"Arkansas\", \".\"], [\"lion\", \".\"], [\"John\", \"B.\", \"Gordon\", \".\", \"of\", \"Georgia\", \".\"], [\"Hon\", \".\"], [\"Lewis\", \"V.\", \"Bogy\", \".\", \"of\", \"Missouri\", \".\"], [\"Hon\", \".\"], [\"Thomas\", \"C.\", \"MeCreery\", \".\", \"of\", \"Kentucky\", \".\"], [\"hon\", \".\"], [\"John\", \"Sherman\", \".\", \"of\", \"Ohio\", \".\"], [\"Hon\", \".\", \"Oliver\", \"P1\", \".\"], [\"Morton\", \".\", \"of\", \"Indiana\", \".\"], [\"Hon\", \".\"], [\"Richard\", \"3\", \".\"], [\"Oglesby\", \".\", \"of\", \"Illinois\", \".\"], [\"Hon\", \".\"], [\"Timothy\", \"0\", \".\"], [\"Howe\", \".\", \"of\", \"Wisconsin\", \".\"], [\"Hon\", \".\"], [\"William\", \"B.\", \"Allison\", \".\"], [\"of\", \"Iowa\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Ingalls\", \".\", \"of\", \"Kansas\", \".\"], [\"hon\", \".\"], [\"Aaron\", \"a.\", \"Sargent\", \".\", \"of\", \"California\", \".\"], [\"hon\", \".\"], [\"John\", \"Il\", \".\"], [\"Mitchell\", \"of\", \"Oregon\", \".\"], [\"hon\", \".\"], [\"John\", \"P.\", \"Jones\", \".\"], [\"otNevada\", \".\"], [\"when\", \"the\", \"name\", \"of\", \"Mr.\", \"Conkling\", \"be\", \"call\", \".\"]], \"tags\": [[\"DT\", \"NNP\", \"MD\", \"VB\", \"DT\", \"NNS\", \"IN\", \"DT\", \"VBN\", \"NNS\", \".\"], [\"DT\", \"NN\", \"VBD\", \"VBN\", \"IN\", \"VBZ\", \":\", \"NN\", \".\"], [\"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\"], [\"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \".\"], [\"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\", \"NNP\", \"NNP\", \".\"], [\"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"CD\", \".\"], [\"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"CD\", \".\"], [\"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\"], [\"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NN\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \".\"], [\"NNP\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"WRB\", \"DT\", \"NN\", \"IN\", \"NNP\", \"NNP\", \"VBD\", \"VBN\", \".\"]], \"deps\": [[\"det\", \"nsubj\", \"aux\", \"ROOT\", \"det\", \"dobj\", \"prep\", \"det\", \"amod\", \"pobj\", \"punct\"], [\"det\", \"nsubjpass\", \"auxpass\", \"ROOT\", \"mark\", \"advcl\", \"punct\", \"nsubj\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"nmod\", \"punct\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"ROOT\", \"nummod\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"ROOT\", \"nummod\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"appos\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"punct\"], [\"advmod\", \"det\", \"nsubjpass\", \"prep\", \"compound\", \"pobj\", \"auxpass\", \"ROOT\", \"punct\"]], \"heads\": [[1, 3, 3, 3, 5, 3, 5, 9, 9, 6, 3], [1, 3, 3, 3, 5, 3, 5, 5, 3], [1, 1, 1, 1, 5, 3, 1], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1, 1, 5, 3, 1], [0, 0], [1, 1, 1, 1, 3, 1], [0, 0], [2, 2, 2, 2], [0, 0, 0], [0, 0], [2, 2, 2, 2, 2, 6, 4, 2], [0, 0], [2, 2, 2, 2, 2, 6, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1], [0, 0, 0, 2, 0], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1, 1, 3, 1], [3, 0, 3, 3, 3], [0, 0, 0, 2, 0], [0, 0], [0, 0, 0], [0, 0, 0, 2, 0], [0, 0], [0, 0, 0], [0, 0, 0, 2, 0], [0, 0], [2, 2, 2, 2], [0, 0, 0], [0, 0], [2, 2, 2, 2, 3, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1], [0, 0, 1, 0], [0, 0], [2, 2, 2, 2], [0, 0], [7, 2, 7, 2, 5, 3, 7, 7, 7]]}\\n', '{\"id\": \"430000002\", \"tokens\": [[\"said\", \":\", \"Mr.\", \"President\", \".\"], [\"owing\", \"to\", \"some\", \"inadvertence\", \"the\", \"credentials\", \"of\", \"the\", \"Senatorelect\", \"from\", \"Now\", \"York\", \"have\", \"not\", \"been\", \"presented\", \"in\", \"this\", \"body\", \".\"], [\"It\", \"is\", \"a\", \"matter\", \"of\", \"public\", \"notoriety\", \"that\", \"he\", \"has\", \"been\", \"elected\", \".\"], [\"and\", \".\"], [\"in\", \"accordance\", \"with\", \"the\", \"usage\", \"of\", \"the\", \"body\", \".\"], [\"I\", \"move\", \"that\", \"the\", \"oath\", \"of\", \"office\", \"be\", \"administered\", \"to\", \"him\", \".\"]], \"lemmas\": [[\"say\", \":\", \"Mr.\", \"President\", \".\"], [\"owe\", \"to\", \"some\", \"inadvertence\", \"the\", \"credential\", \"of\", \"the\", \"Senatorelect\", \"from\", \"Now\", \"York\", \"have\", \"not\", \"be\", \"present\", \"in\", \"this\", \"body\", \".\"], [\"it\", \"be\", \"a\", \"matter\", \"of\", \"public\", \"notoriety\", \"that\", \"he\", \"have\", \"be\", \"elect\", \".\"], [\"and\", \".\"], [\"in\", \"accordance\", \"with\", \"the\", \"usage\", \"of\", \"the\", \"body\", \".\"], [\"I\", \"move\", \"that\", \"the\", \"oath\", \"of\", \"office\", \"be\", \"administer\", \"to\", \"he\", \".\"]], \"tags\": [[\"VBD\", \":\", \"NNP\", \"NNP\", \".\"], [\"VBG\", \"IN\", \"DT\", \"NN\", \"DT\", \"NNS\", \"IN\", \"DT\", \"NNP\", \"IN\", \"NNP\", \"NNP\", \"VBP\", \"RB\", \"VBN\", \"VBN\", \"IN\", \"DT\", \"NN\", \".\"], [\"PRP\", \"VBZ\", \"DT\", \"NN\", \"IN\", \"JJ\", \"NN\", \"WDT\", \"PRP\", \"VBZ\", \"VBN\", \"VBN\", \".\"], [\"CC\", \".\"], [\"IN\", \"NN\", \"IN\", \"DT\", \"NN\", \"IN\", \"DT\", \"NN\", \".\"], [\"PRP\", \"VBP\", \"IN\", \"DT\", \"NN\", \"IN\", \"NN\", \"VB\", \"VBN\", \"IN\", \"PRP\", \".\"]], \"deps\": [[\"ROOT\", \"punct\", \"compound\", \"nsubj\", \"punct\"], [\"prep\", \"prep\", \"det\", \"pobj\", \"det\", \"nsubjpass\", \"prep\", \"det\", \"pobj\", \"prep\", \"pcomp\", \"pobj\", \"aux\", \"neg\", \"auxpass\", \"ROOT\", \"prep\", \"det\", \"pobj\", \"punct\"], [\"nsubj\", \"ROOT\", \"det\", \"attr\", \"prep\", \"amod\", \"pobj\", \"mark\", \"nsubjpass\", \"aux\", \"auxpass\", \"relcl\", \"punct\"], [\"ROOT\", \"punct\"], [\"ROOT\", \"pobj\", \"prep\", \"det\", \"pobj\", \"prep\", \"det\", \"pobj\", \"punct\"], [\"nsubj\", \"ROOT\", \"mark\", \"det\", \"nsubjpass\", \"prep\", \"pobj\", \"auxpass\", \"ccomp\", \"prep\", \"pobj\", \"punct\"]], \"heads\": [[0, 0, 3, 0, 0], [15, 0, 3, 1, 5, 15, 5, 8, 6, 5, 9, 9, 15, 15, 15, 15, 15, 18, 16, 15], [1, 1, 3, 1, 3, 6, 4, 11, 11, 11, 11, 3, 1], [0, 0], [0, 0, 1, 4, 2, 4, 7, 5, 0], [1, 1, 8, 4, 8, 4, 5, 8, 1, 8, 9, 1]]}\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('./Congress/hein-bound_parsed/speeches_043.txt', 'r') as f:\n",
    "    # Read the first line of the file\n",
    "    lines = f.readlines()\n",
    "print(lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c5591c-8831-4e8a-b27e-70daa04d845d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"430000001\", \"tokens\": [[\"The\", \"Secretary\", \"will\", \"read\", \"the\", \"names\", \"of\", \"the\", \"newlyelected\", \"Senators\", \".\"], [\"The\", \"list\", \"was\", \"read\", \"as\", \"follows\", \":\", \"lion\", \".\"], [\"Bainbridge\", \"Wadleigh\", \".\", \"of\", \"New\", \"Hampshire\", \".\"], [\"Hon\", \".\"], [\"Justin\", \"S.\", \"Morrill\", \".\", \"of\", \"Vermont\", \".\"], [\"Hon\", \".\"], [\"Orris\", \"S.\", \"Ferry\", \".\", \"of\", \"Connecticut\", \".\"], [\"Hon\", \".\"], [\"Roscoe\", \"Coukling\", \".\", \"of\", \"New\", \"York\", \".\"], [\"Hon\", \".\"], [\"Simon\", \"Cameron\", \".\", \"of\", \"Pennsylvania\", \".\"], [\"lan\", \".\"], [\"George\", \"h.\", \"Dennis\", \".\"], [\"of\", \"Maryland\", \".\"], [\"Hon\", \".\"], [\"Augustus\", \"S.\", \"Merrimon\", \".\", \"of\", \"North\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Patterson\", \".\", \"of\", \"South\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"Simon\", \"B.\", \"Conover\", \".\", \"of\", \"Florida\", \".\"], [\"Ion\", \".\"], [\"George\", \"E.\", \"Spencer\", \".\", \"of\", \"Alabama\", \".\"], [\"H10a\", \".\"], [\"Stephen\", \"WV\", \".\"], [\"Dorsey\", \".\", \"of\", \"Arkansas\", \".\"], [\"lion\", \".\"], [\"John\", \"B.\", \"Gordon\", \".\", \"of\", \"Georgia\", \".\"], [\"Hon\", \".\"], [\"Lewis\", \"V.\", \"Bogy\", \".\", \"of\", \"Missouri\", \".\"], [\"Hon\", \".\"], [\"Thomas\", \"C.\", \"MeCreery\", \".\", \"of\", \"Kentucky\", \".\"], [\"Hon\", \".\"], [\"John\", \"Sherman\", \".\", \"of\", \"Ohio\", \".\"], [\"Hon\", \".\", \"Oliver\", \"P1\", \".\"], [\"Morton\", \".\", \"of\", \"Indiana\", \".\"], [\"Hon\", \".\"], [\"Richard\", \"3\", \".\"], [\"Oglesby\", \".\", \"of\", \"Illinois\", \".\"], [\"Hon\", \".\"], [\"Timothy\", \"0\", \".\"], [\"Howe\", \".\", \"of\", \"Wisconsin\", \".\"], [\"Hon\", \".\"], [\"William\", \"B.\", \"Allison\", \".\"], [\"of\", \"Iowa\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Ingalls\", \".\", \"of\", \"Kansas\", \".\"], [\"Hon\", \".\"], [\"Aaron\", \"A.\", \"Sargent\", \".\", \"of\", \"California\", \".\"], [\"Hon\", \".\"], [\"John\", \"Il\", \".\"], [\"Mitchell\", \"of\", \"Oregon\", \".\"], [\"Hon\", \".\"], [\"John\", \"P.\", \"Jones\", \".\"], [\"otNevada\", \".\"], [\"When\", \"the\", \"name\", \"of\", \"Mr.\", \"Conkling\", \"was\", \"called\", \".\"]], \"lemmas\": [[\"the\", \"Secretary\", \"will\", \"read\", \"the\", \"name\", \"of\", \"the\", \"newlyelecte\", \"senator\", \".\"], [\"the\", \"list\", \"be\", \"read\", \"as\", \"follow\", \":\", \"lion\", \".\"], [\"Bainbridge\", \"Wadleigh\", \".\", \"of\", \"New\", \"Hampshire\", \".\"], [\"hon\", \".\"], [\"Justin\", \"S.\", \"Morrill\", \".\", \"of\", \"Vermont\", \".\"], [\"hon\", \".\"], [\"Orris\", \"S.\", \"Ferry\", \".\", \"of\", \"Connecticut\", \".\"], [\"hon\", \".\"], [\"Roscoe\", \"Coukling\", \".\", \"of\", \"New\", \"York\", \".\"], [\"hon\", \".\"], [\"Simon\", \"Cameron\", \".\", \"of\", \"Pennsylvania\", \".\"], [\"lan\", \".\"], [\"George\", \"h.\", \"Dennis\", \".\"], [\"of\", \"Maryland\", \".\"], [\"hon\", \".\"], [\"Augustus\", \"S.\", \"Merrimon\", \".\", \"of\", \"North\", \"Carolina\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Patterson\", \".\", \"of\", \"South\", \"Carolina\", \".\"], [\"hon\", \".\"], [\"Simon\", \"B.\", \"Conover\", \".\", \"of\", \"Florida\", \".\"], [\"Ion\", \".\"], [\"George\", \"E.\", \"Spencer\", \".\", \"of\", \"Alabama\", \".\"], [\"H10a\", \".\"], [\"Stephen\", \"WV\", \".\"], [\"Dorsey\", \".\", \"of\", \"Arkansas\", \".\"], [\"lion\", \".\"], [\"John\", \"B.\", \"Gordon\", \".\", \"of\", \"Georgia\", \".\"], [\"Hon\", \".\"], [\"Lewis\", \"V.\", \"Bogy\", \".\", \"of\", \"Missouri\", \".\"], [\"Hon\", \".\"], [\"Thomas\", \"C.\", \"MeCreery\", \".\", \"of\", \"Kentucky\", \".\"], [\"hon\", \".\"], [\"John\", \"Sherman\", \".\", \"of\", \"Ohio\", \".\"], [\"Hon\", \".\", \"Oliver\", \"P1\", \".\"], [\"Morton\", \".\", \"of\", \"Indiana\", \".\"], [\"Hon\", \".\"], [\"Richard\", \"3\", \".\"], [\"Oglesby\", \".\", \"of\", \"Illinois\", \".\"], [\"Hon\", \".\"], [\"Timothy\", \"0\", \".\"], [\"Howe\", \".\", \"of\", \"Wisconsin\", \".\"], [\"Hon\", \".\"], [\"William\", \"B.\", \"Allison\", \".\"], [\"of\", \"Iowa\", \".\"], [\"Hon\", \".\"], [\"John\", \"J.\", \"Ingalls\", \".\", \"of\", \"Kansas\", \".\"], [\"hon\", \".\"], [\"Aaron\", \"a.\", \"Sargent\", \".\", \"of\", \"California\", \".\"], [\"hon\", \".\"], [\"John\", \"Il\", \".\"], [\"Mitchell\", \"of\", \"Oregon\", \".\"], [\"hon\", \".\"], [\"John\", \"P.\", \"Jones\", \".\"], [\"otNevada\", \".\"], [\"when\", \"the\", \"name\", \"of\", \"Mr.\", \"Conkling\", \"be\", \"call\", \".\"]], \"tags\": [[\"DT\", \"NNP\", \"MD\", \"VB\", \"DT\", \"NNS\", \"IN\", \"DT\", \"VBN\", \"NNS\", \".\"], [\"DT\", \"NN\", \"VBD\", \"VBN\", \"IN\", \"VBZ\", \":\", \"NN\", \".\"], [\"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\"], [\"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \".\"], [\"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\", \"NNP\", \"NNP\", \".\"], [\"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"CD\", \".\"], [\"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"CD\", \".\"], [\"NNP\", \".\", \"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\"], [\"IN\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"NN\", \".\"], [\"NNP\", \"NN\", \"NNP\", \"NNP\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \".\"], [\"NNP\", \"IN\", \"NNP\", \".\"], [\"UH\", \".\"], [\"NNP\", \"NNP\", \"NNP\", \".\"], [\"NNP\", \".\"], [\"WRB\", \"DT\", \"NN\", \"IN\", \"NNP\", \"NNP\", \"VBD\", \"VBN\", \".\"]], \"deps\": [[\"det\", \"nsubj\", \"aux\", \"ROOT\", \"det\", \"dobj\", \"prep\", \"det\", \"amod\", \"pobj\", \"punct\"], [\"det\", \"nsubjpass\", \"auxpass\", \"ROOT\", \"mark\", \"advcl\", \"punct\", \"nsubj\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"compound\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"nmod\", \"punct\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"ROOT\", \"nummod\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"ROOT\", \"nummod\", \"punct\"], [\"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"appos\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"prep\", \"pobj\", \"punct\"], [\"ROOT\", \"punct\"], [\"compound\", \"compound\", \"ROOT\", \"punct\"], [\"ROOT\", \"punct\"], [\"advmod\", \"det\", \"nsubjpass\", \"prep\", \"compound\", \"pobj\", \"auxpass\", \"ROOT\", \"punct\"]], \"heads\": [[1, 3, 3, 3, 5, 3, 5, 9, 9, 6, 3], [1, 3, 3, 3, 5, 3, 5, 5, 3], [1, 1, 1, 1, 5, 3, 1], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1, 1, 5, 3, 1], [0, 0], [1, 1, 1, 1, 3, 1], [0, 0], [2, 2, 2, 2], [0, 0, 0], [0, 0], [2, 2, 2, 2, 2, 6, 4, 2], [0, 0], [2, 2, 2, 2, 2, 6, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1], [0, 0, 0, 2, 0], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1, 1, 3, 1], [3, 0, 3, 3, 3], [0, 0, 0, 2, 0], [0, 0], [0, 0, 0], [0, 0, 0, 2, 0], [0, 0], [0, 0, 0], [0, 0, 0, 2, 0], [0, 0], [2, 2, 2, 2], [0, 0, 0], [0, 0], [2, 2, 2, 2, 3, 4, 2], [0, 0], [2, 2, 2, 2, 2, 4, 2], [0, 0], [1, 1, 1], [0, 0, 1, 0], [0, 0], [2, 2, 2, 2], [0, 0], [7, 2, 7, 2, 5, 3, 7, 7, 7]]}\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8f363-1562-44af-87d5-15c50695d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nohup python script.py --hein-bound-dir /data/hein-bound-tokenized --hein-daily-dir /data/hein-daily-tokenized --first 43 --last 114 --max 375 --keep-boundaries --replace-periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c0acd6-1b10-4e39-a769-5251c8246fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         speech_id  year filenum\n",
      "0        430000001  1873     043\n",
      "1        430000002  1873     043\n",
      "2        430000003  1873     043\n",
      "3        430000004  1873     043\n",
      "4        430000005  1873     043\n",
      "...            ...   ...     ...\n",
      "179263  1110179264  2010     111\n",
      "179264  1110179265  2010     111\n",
      "179265  1110179266  2010     111\n",
      "179266  1110179267  2010     111\n",
      "179267  1110179268  2010     111\n",
      "\n",
      "[17395884 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "data_dir = \"./Congress/hein-bound/\"\n",
    "\n",
    "# Create an empty dataframe to store the results\n",
    "df_results = pd.DataFrame()\n",
    "for i in range(43,112):\n",
    "    if i < 100:\n",
    "        n = \"0\"+str(i)\n",
    "    else:\n",
    "        n = str(i)\n",
    "    descr = pd.read_csv(os.path.join(data_dir, f'descr_{n}.txt'), \n",
    "                          encoding=\"ISO-8859-1\", \n",
    "                          sep=\"|\")\n",
    "    dates = pd.to_datetime(descr['date'], format=\"%Y%m%d\")\n",
    "    descr['year'] = dates.dt.year\n",
    "    descr = descr.assign(filenum=n)\n",
    "    df_selected = descr.loc[:, [\"speech_id\", \"year\",\"filenum\"]]\n",
    "    df_results = pd.concat([df_results, df_selected])\n",
    "\n",
    "\n",
    "# Print the resulting dataframe\n",
    "print(df_results)\n",
    "df_results.to_csv(os.path.join('./Congress/',\"congressyear.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da48b32-3a88-491f-9bde-f63b56c985a8",
   "metadata": {},
   "source": [
    "# start of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773cae90-0a8b-4267-b9f1-6d1ec36b1fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pro_dir = './Congress'\n",
    "data_dir = \"./Congress/hein-bound/\"\n",
    "text_dir = \"./Congress/hein-bound_parsed/\"\n",
    "model_dir = \"./Congress/model/\"\n",
    "\n",
    "dfresults = pd.read_csv(os.path.join(pro_dir,'congressyear.csv'), dtype={'filenum': str} )\n",
    "# dfresults['year'].hist(bins = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26208687-3ee6-4c95-a7a2-824b6b6f349a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech_id</th>\n",
       "      <th>year</th>\n",
       "      <th>filenum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430000001</td>\n",
       "      <td>1873</td>\n",
       "      <td>043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430000002</td>\n",
       "      <td>1873</td>\n",
       "      <td>043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>430000003</td>\n",
       "      <td>1873</td>\n",
       "      <td>043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430000004</td>\n",
       "      <td>1873</td>\n",
       "      <td>043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430000005</td>\n",
       "      <td>1873</td>\n",
       "      <td>043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395879</th>\n",
       "      <td>1110179264</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395880</th>\n",
       "      <td>1110179265</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395881</th>\n",
       "      <td>1110179266</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395882</th>\n",
       "      <td>1110179267</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395883</th>\n",
       "      <td>1110179268</td>\n",
       "      <td>2010</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17395884 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           speech_id  year filenum\n",
       "0          430000001  1873     043\n",
       "1          430000002  1873     043\n",
       "2          430000003  1873     043\n",
       "3          430000004  1873     043\n",
       "4          430000005  1873     043\n",
       "...              ...   ...     ...\n",
       "17395879  1110179264  2010     111\n",
       "17395880  1110179265  2010     111\n",
       "17395881  1110179266  2010     111\n",
       "17395882  1110179267  2010     111\n",
       "17395883  1110179268  2010     111\n",
       "\n",
       "[17395884 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9e475da-2d76-428b-9744-54eb0806028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875 ['043', '044']\n",
      "1877 ['045', '044']\n",
      "1879 ['045', '046']\n",
      "1881 ['047', '046']\n",
      "1883 ['048', '047']\n",
      "1885 ['048', '049']\n",
      "1887 ['050', '049']\n",
      "1889 ['051', '050']\n",
      "1891 ['052', '051']\n",
      "1893 ['052', '053']\n",
      "1895 ['053', '054']\n",
      "1897 ['055', '054']\n",
      "1899 ['055', '056']\n",
      "1901 ['056', '057']\n",
      "1903 ['057', '058']\n",
      "1905 ['059', '058']\n",
      "1907 ['059', '060']\n",
      "1909 ['061', '060']\n",
      "1911 ['061', '062']\n",
      "1913 ['063', '062']\n",
      "1915 ['063', '064']\n",
      "1917 ['064', '065']\n",
      "1919 ['066', '065']\n",
      "1921 ['067', '066']\n",
      "1923 ['067', '068']\n",
      "1925 ['068', '069']\n",
      "1927 ['070', '069']\n",
      "1929 ['070', '071']\n",
      "1931 ['072', '071']\n",
      "1933 ['073', '072']\n",
      "1941 ['077', '076']\n",
      "1951 ['081', '082']\n",
      "1971 ['092', '091']\n",
      "2009 ['111', '110']\n"
     ]
    }
   ],
   "source": [
    "for year in range(1873,2011):\n",
    "    df_new = dfresults.loc[dfresults['year']==year]\n",
    "    if len(list(set(df_new['filenum'])))>1:\n",
    "        print(year, list(set(df_new['filenum'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6b4ec2a-5de2-4fc6-bd89-f58af79e5c12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"infile\": \"speeches_053\", \"id\": \"530000001\", \"sents\": [\"Senators. deeply impressed with a sense of its responsibilities and of its dignities.\", \"I now enter upon the discharge of the duties of the high office to which I have been called.\", \"I am not unmindful of the fact that among the occupants of this chair during the one hundred and four years of our constitutional history have been statesmen eminent alike for their talents and for their tireless devotion to public duty.\", \"Adams.\", \"Jefferson. and Calhoun honored its incumbency during the early days of the Republic. while Arthur.\", \"Hendricks. and Morton have at a later period of our history shed luster upon the office of President of the most august deliberative assembly known to men.\", \"I assume the duties of the great trust confided to me with no feeling of selfconfidence. but rather with that of grave distrust of my ability satisfactorily to meet its requirements.\", \"I may be pardoned for saying that it shall be my earnest endeavor to discharge the important duties which lie before me with no less of impartiality and courtesy than of firmness and fidelity.\", \"Earnestly invoking the cooperation. the forbearance. the charity of each of its members.\", \"I now enter upon my duties as Presiding Officer of the Senate.\", \"The Secretary of the Senate will read the proclamation of the President of the United States convening the Senate in extraordinary session.\"], \"tokens\": [[\"Senators\", \".\", \"deeply\", \"impressed\", \"with\", \"a\", \"sense\", \"of\", \"its\", \"responsibilities\", \"and\", \"of\", \"its\", \"dignities\", \".\"], [\"I\", \"now\", \"enter\", \"upon\", \"the\", \"discharge\", \"of\", \"the\", \"duties\", \"of\", \"the\", \"high\", \"office\", \"to\", \"which\", \"I\", \"have\", \"been\", \"called\", \".\"], [\"I\", \"am\", \"not\", \"unmindful\", \"of\", \"the\", \"fact\", \"that\", \"among\", \"the\", \"occupants\", \"of\", \"this\", \"chair\", \"during\", \"the\", \"one\", \"hundred\", \"and\", \"four\", \"years\", \"of\", \"our\", \"constitutional\", \"history\", \"have\", \"been\", \"statesmen\", \"eminent\", \"alike\", \"for\", \"their\", \"talents\", \"and\", \"for\", \"their\", \"tireless\", \"devotion\", \"to\", \"public\", \"duty\", \".\"], [\"Adams\", \".\"], [\"Jefferson\", \".\", \"and\", \"Calhoun\", \"honored\", \"its\", \"incumbency\", \"during\", \"the\", \"early\", \"days\", \"of\", \"the\", \"Republic\", \".\", \"while\", \"Arthur\", \".\"], [\"Hendricks\", \".\", \"and\", \"Morton\", \"have\", \"at\", \"a\", \"later\", \"period\", \"of\", \"our\", \"history\", \"shed\", \"luster\", \"upon\", \"the\", \"office\", \"of\", \"President\", \"of\", \"the\", \"most\", \"august\", \"deliberative\", \"assembly\", \"known\", \"to\", \"men\", \".\"], [\"I\", \"assume\", \"the\", \"duties\", \"of\", \"the\", \"great\", \"trust\", \"confided\", \"to\", \"me\", \"with\", \"no\", \"feeling\", \"of\", \"selfconfidence\", \".\", \"but\", \"rather\", \"with\", \"that\", \"of\", \"grave\", \"distrust\", \"of\", \"my\", \"ability\", \"satisfactorily\", \"to\", \"meet\", \"its\", \"requirements\", \".\"], [\"I\", \"may\", \"be\", \"pardoned\", \"for\", \"saying\", \"that\", \"it\", \"shall\", \"be\", \"my\", \"earnest\", \"endeavor\", \"to\", \"discharge\", \"the\", \"important\", \"duties\", \"which\", \"lie\", \"before\", \"me\", \"with\", \"no\", \"less\", \"of\", \"impartiality\", \"and\", \"courtesy\", \"than\", \"of\", \"firmness\", \"and\", \"fidelity\", \".\"], [\"Earnestly\", \"invoking\", \"the\", \"cooperation\", \".\", \"the\", \"forbearance\", \".\", \"the\", \"charity\", \"of\", \"each\", \"of\", \"its\", \"members\", \".\"], [\"I\", \"now\", \"enter\", \"upon\", \"my\", \"duties\", \"as\", \"Presiding\", \"Officer\", \"of\", \"the\", \"Senate\", \".\"], [\"The\", \"Secretary\", \"of\", \"the\", \"Senate\", \"will\", \"read\", \"the\", \"proclamation\", \"of\", \"the\", \"President\", \"of\", \"the\", \"United\", \"States\", \"convening\", \"the\", \"Senate\", \"in\", \"extraordinary\", \"session\", \".\"]]}\\n',\n",
       " '{\"infile\": \"speeches_053\", \"id\": \"530000002\", \"sents\": [\"The names of the newly elected Senators whose credentials are on file will now be called by the Secretary. and they will come forward and receive the oath of office four at a time.\"], \"tokens\": [[\"The\", \"names\", \"of\", \"the\", \"newly\", \"elected\", \"Senators\", \"whose\", \"credentials\", \"are\", \"on\", \"file\", \"will\", \"now\", \"be\", \"called\", \"by\", \"the\", \"Secretary\", \".\", \"and\", \"they\", \"will\", \"come\", \"forward\", \"and\", \"receive\", \"the\", \"oath\", \"of\", \"office\", \"four\", \"at\", \"a\", \"time\", \".\"]]}\\n',\n",
       " '{\"infile\": \"speeches_053\", \"id\": \"530000003\", \"sents\": [\"I present the credentials of John Martin. recently elected a Senator from Kansas for the term ending March 3. 1895. and ask that they be read and lie on the table for the present.\"], \"tokens\": [[\"I\", \"present\", \"the\", \"credentials\", \"of\", \"John\", \"Martin\", \".\", \"recently\", \"elected\", \"a\", \"Senator\", \"from\", \"Kansas\", \"for\", \"the\", \"term\", \"ending\", \"March\", \"3\", \".\", \"1895\", \".\", \"and\", \"ask\", \"that\", \"they\", \"be\", \"read\", \"and\", \"lie\", \"on\", \"the\", \"table\", \"for\", \"the\", \"present\", \".\"]]}\\n']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infile = f'./Congress/hein-bound_parsed/speeches_053.jsonlist'\n",
    "with open(infile, 'r') as f:\n",
    "    # Read the lines from the file\n",
    "    lines = f.readlines()\n",
    "lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f6fc6c5a-bf98-47c3-aa3e-72476ac0f39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_new = dfresults.loc[dfresults['year']==year]\n",
    "allline = []\n",
    "infile = f'./Congress/hein-bound_parsed/speeches_053.jsonlist'\n",
    "with open(infile, 'r') as f:\n",
    "    # Read the lines from the file\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:100]:\n",
    "        jline = json.loads(line)\n",
    "        if int(jline['id']) in df_new['speech_id'].to_list():\n",
    "            print(int(jline['id']))\n",
    "            # print(jline['tokens'])\n",
    "            allline.extend(jline['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2443ad85-4448-4d19-8b27-3d5c24a32e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530000100"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(jline['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f5bbd3a-7f46-492f-97aa-a838d169c7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "053\n",
      "054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "for year in range(1895,1896):\n",
    "    df_new = dfresults.loc[dfresults['year']==year]\n",
    "    allline = []\n",
    "    for filenum in list(set(df_new['filenum'])):\n",
    "        print(filenum)\n",
    "        infile = f'./Congress/hein-bound_parsed/speeches_{filenum}.jsonlist'\n",
    "        with open(infile, 'r') as f:\n",
    "            # Read the lines from the file\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                jline = json.loads(line)\n",
    "                if int(jline['id']) in df_new['speech_id'].to_list():\n",
    "                    # print(int(jline['id']))\n",
    "                    # print(jline['tokens'])\n",
    "                    allline.extend(jline['tokens'])\n",
    "         \n",
    "            \n",
    "len(alldata)\n",
    "# model_1 = Word2Vec(,workers=cores-1)\n",
    "# model_1.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e88beb1c-9b3b-4d67-9dea-824298696eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Congress/model/1999.model'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldata)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 3    # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_dir = \"./Congress/model/\"\n",
    "\n",
    "model_2 = Word2Vec(allline,\n",
    "            sg=1,\n",
    "            seed=1,\n",
    "            workers=num_workers,\n",
    "            \n",
    "            min_count=min_word_count,\n",
    "            window=context,\n",
    "            sample=downsampling)\n",
    "model_file = os.path.join(model_dir,'1895.model')\n",
    "model_1.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6bcc839d-65bf-460b-b70d-308cd5fb7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alldata)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 3    # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model_dir = \"./Congress/model/\"\n",
    "\n",
    "model_1 = Word2Vec(allline,\n",
    "            sg=1,\n",
    "            seed=1,\n",
    "            workers=num_workers,\n",
    "            \n",
    "            min_count=min_word_count,\n",
    "            window=context,\n",
    "            sample=downsampling)\n",
    "model_file = os.path.join(model_dir,'1895.model')\n",
    "model_1.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e71c7ad8-2ddc-4f20-8778-ac3df100915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'president'\t'king'\t0.59\n",
      "'president'\t'law'\t0.30\n",
      "'president'\t'air'\t0.41\n",
      "'president'\t'car'\t0.59\n",
      "'president'\t'marriage'\t0.54\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('president', 'king'),   # a minivan is a kind of car\n",
    "    ('president', 'law'),   # still a wheeled vehicle\n",
    "    ('president', 'air'),  # ok, no wheels, but still a vehicle\n",
    "    ('president', 'car'),    # ... and so on\n",
    "    ('president', 'marriage'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model_1.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "858bd19a-ac60-40ee-a2fe-556367d4f183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "land\n"
     ]
    }
   ],
   "source": [
    "print(model_1.wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7380b568-bbc4-4466-b181-2782d9f73b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "043\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "\n",
    "pro_dir = './Congress'\n",
    "data_dir = \"./Congress/hein-bound/\"\n",
    "text_dir = \"./Congress/hein-bound_parsed/\"\n",
    "model_dir = \"./Congress/model/\"\n",
    "\n",
    "dfresults = pd.read_csv(os.path.join(pro_dir,'congressyear.csv'), dtype={'filenum': str} )\n",
    "\n",
    "year = 1874\n",
    "\n",
    "model_file = os.path.join(model_dir,str(year)+'.model')\n",
    "if not os.path.exists(model_file):\n",
    "    df_new = dfresults.loc[dfresults['year']==year]\n",
    "    allline = []\n",
    "    for filenum in list(set(df_new['filenum'])):\n",
    "        print(filenum)\n",
    "        infile = f'./Congress/hein-bound_parsed/speeches_{filenum}.jsonlist'\n",
    "        with open(infile, 'r') as f:\n",
    "            # Read the lines from the file\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                jline = json.loads(line)\n",
    "                if int(jline['id']) in df_new['speech_id'].to_list():\n",
    "                    # print(int(jline['id']))\n",
    "                    # print(jline['tokens'])\n",
    "                    allline.extend(jline['tokens'])\n",
    "    # Set values for various parameters\n",
    "    num_features = 100    # Word vector dimensionality\n",
    "    min_word_count = 1    # Minimum word count\n",
    "    num_workers = 4       # Number of threads to run in parallel\n",
    "    context = 10          # Context window size\n",
    "    downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "    model_1 = Word2Vec(allline,\n",
    "                sg=1,\n",
    "                seed=1,\n",
    "                workers=num_workers,\n",
    "\n",
    "                min_count=min_word_count,\n",
    "                window=context,\n",
    "                sample=downsampling)\n",
    "    model_file = os.path.join(model_dir,str(year)+'.model')\n",
    "    model_1.save(model_file)\n",
    "else:\n",
    "\tprint(str(model_file), \" existed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb32ab0f-0f43-4318-8c52-0c245d80ae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speeches_043.txt: 440.00 MB\n",
      "speeches_044.txt: 414.86 MB\n",
      "speeches_045.txt: 410.46 MB\n",
      "speeches_046.txt: 503.45 MB\n",
      "speeches_047.txt: 604.26 MB\n",
      "speeches_048.txt: 414.42 MB\n",
      "speeches_049.txt: 498.79 MB\n",
      "speeches_070.txt: 449.59 MB\n",
      "speeches_060.txt: 381.74 MB\n",
      "speeches_050.txt: 532.83 MB\n",
      "speeches_080.txt: 752.65 MB\n",
      "speeches_061.txt: 689.08 MB\n",
      "speeches_100.txt: 1636.66 MB\n",
      "speeches_071.txt: 766.32 MB\n",
      "speeches_051.txt: 682.34 MB\n",
      "speeches_090.txt: 1629.30 MB\n",
      "speeches_081.txt: 1066.31 MB\n",
      "speeches_052.txt: 409.06 MB\n",
      "speeches_072.txt: 648.54 MB\n",
      "speeches_062.txt: 821.20 MB\n",
      "speeches_053.txt: 666.70 MB\n",
      "speeches_082.txt: 782.93 MB\n",
      "speeches_073.txt: 559.05 MB\n",
      "speeches_101.txt: 1584.55 MB\n",
      "speeches_054.txt: 418.15 MB\n",
      "speeches_091.txt: 1800.18 MB\n",
      "speeches_074.txt: 706.30 MB\n",
      "speeches_063.txt: 1166.57 MB\n",
      "speeches_055.txt: 548.31 MB\n",
      "speeches_083.txt: 1037.30 MB\n",
      "speeches_056.txt: 443.93 MB\n",
      "speeches_075.txt: 648.56 MB\n",
      "speeches_102.txt: 1644.02 MB\n",
      "speeches_064.txt: 756.07 MB\n",
      "speeches_057.txt: 404.49 MB\n",
      "speeches_092.txt: 1693.42 MB\n",
      "speeches_084.txt: 1063.65 MB\n",
      "speeches_058.txt: 349.51 MB\n",
      "speeches_076.txt: 812.13 MB\n",
      "speeches_059.txt: 492.06 MB\n",
      "speeches_065.txt: 1020.97 MB\n",
      "speeches_103.txt: 1573.60 MB\n",
      "speeches_077.txt: 727.35 MB\n",
      "speeches_085.txt: 1320.93 MB\n",
      "speeches_093.txt: 1803.11 MB\n",
      "speeches_066.txt: 882.75 MB\n",
      "speeches_078.txt: 750.99 MB\n",
      "speeches_104.txt: 1815.35 MB\n",
      "speeches_086.txt: 1373.54 MB\n",
      "speeches_079.txt: 844.97 MB\n",
      "speeches_067.txt: 1111.84 MB\n",
      "speeches_094.txt: 1757.74 MB\n",
      "speeches_068.txt: 592.32 MB\n",
      "speeches_105.txt: 1500.41 MB\n",
      "speeches_087.txt: 1470.11 MB\n",
      "speeches_069.txt: 583.96 MB\n",
      "speeches_095.txt: 1799.56 MB\n",
      "speeches_106.txt: 1554.52 MB\n",
      "speeches_088.txt: 1494.29 MB\n",
      "speeches_107.txt: 1348.59 MB\n",
      "speeches_096.txt: 1689.32 MB\n",
      "speeches_089.txt: 1651.23 MB\n",
      "speeches_110.txt: 1542.18 MB\n",
      "speeches_108.txt: 1460.64 MB\n",
      "speeches_097.txt: 1504.35 MB\n",
      "speeches_111.txt: 1340.05 MB\n",
      "speeches_109.txt: 1399.25 MB\n",
      "speeches_098.txt: 1536.00 MB\n",
      "speeches_099.txt: 1655.43 MB\n",
      "speeches_043.jsonlist: 173.91 MB\n",
      ".ipynb_checkpoints: 0.00 MB\n",
      "speeches_058.jsonlist: 140.11 MB\n",
      "speeches_044.jsonlist: 164.60 MB\n",
      "speeches_045.jsonlist: 163.78 MB\n",
      "speeches_057.jsonlist: 161.68 MB\n",
      "speeches_060.jsonlist: 153.59 MB\n",
      "speeches_048.jsonlist: 166.40 MB\n",
      "speeches_052.jsonlist: 164.18 MB\n",
      "speeches_054.jsonlist: 167.37 MB\n",
      "speeches_056.jsonlist: 177.61 MB\n",
      "speeches_070.jsonlist: 181.87 MB\n",
      "speeches_046.jsonlist: 201.07 MB\n",
      "speeches_049.jsonlist: 199.75 MB\n",
      "speeches_059.jsonlist: 197.61 MB\n",
      "speeches_050.jsonlist: 213.92 MB\n",
      "speeches_055.jsonlist: 219.26 MB\n",
      "speeches_073.jsonlist: 224.99 MB\n",
      "speeches_047.jsonlist: 241.29 MB\n",
      "speeches_068.jsonlist: 238.14 MB\n",
      "speeches_069.jsonlist: 235.82 MB\n",
      "speeches_072.jsonlist: 261.38 MB\n",
      "speeches_053.jsonlist: 266.52 MB\n",
      "speeches_075.jsonlist: 262.03 MB\n",
      "speeches_051.jsonlist: 272.90 MB\n",
      "speeches_074.jsonlist: 284.51 MB\n",
      "speeches_080.jsonlist: 302.55 MB\n",
      "speeches_077.jsonlist: 292.11 MB\n",
      "speeches_061.jsonlist: 277.36 MB\n",
      "speeches_078.jsonlist: 302.43 MB\n",
      "speeches_082.jsonlist: 315.47 MB\n",
      "speeches_079.jsonlist: 339.77 MB\n",
      "speeches_064.jsonlist: 303.77 MB\n",
      "speeches_071.jsonlist: 309.74 MB\n",
      "speeches_076.jsonlist: 327.36 MB\n",
      "speeches_062.jsonlist: 330.59 MB\n",
      "speeches_084.jsonlist: 427.09 MB\n",
      "speeches_066.jsonlist: 354.89 MB\n",
      "speeches_083.jsonlist: 416.63 MB\n",
      "speeches_081.jsonlist: 429.54 MB\n",
      "speeches_107.jsonlist: 536.71 MB\n",
      "speeches_111.jsonlist: 531.63 MB\n",
      "speeches_109.jsonlist: 556.78 MB\n",
      "speeches_065.jsonlist: 410.01 MB\n",
      "speeches_108.jsonlist: 580.85 MB\n",
      "speeches_067.jsonlist: 445.69 MB\n",
      "speeches_085.jsonlist: 530.51 MB\n",
      "speeches_105.jsonlist: 596.73 MB\n",
      "speeches_110.jsonlist: 612.01 MB\n",
      "speeches_106.jsonlist: 618.72 MB\n",
      "speeches_086.jsonlist: 551.66 MB\n",
      "speeches_097.jsonlist: 606.18 MB\n",
      "speeches_087.jsonlist: 591.46 MB\n",
      "speeches_063.jsonlist: 469.09 MB\n",
      "speeches_103.jsonlist: 627.41 MB\n",
      "speeches_098.jsonlist: 619.05 MB\n",
      "speeches_088.jsonlist: 600.43 MB\n",
      "speeches_101.jsonlist: 635.57 MB\n",
      "speeches_102.jsonlist: 658.14 MB\n",
      "speeches_100.jsonlist: 658.42 MB\n",
      "speeches_099.jsonlist: 664.74 MB\n",
      "speeches_089.jsonlist: 665.02 MB\n",
      "speeches_090.jsonlist: 656.38 MB\n",
      "speeches_092.jsonlist: 682.53 MB\n",
      "speeches_096.jsonlist: 681.78 MB\n",
      "speeches_104.jsonlist: 720.52 MB\n",
      "speeches_091.jsonlist: 724.48 MB\n",
      "speeches_094.jsonlist: 710.53 MB\n",
      "speeches_093.jsonlist: 728.35 MB\n",
      "speeches_095.jsonlist: 727.20 MB\n"
     ]
    }
   ],
   "source": [
    "# List the files in the folder\n",
    "files = os.listdir(text_dir)\n",
    "\n",
    "# Loop through the files in the folder\n",
    "for file in files:\n",
    "    # Get the full file path\n",
    "    file_path = os.path.join(text_dir, file)\n",
    "\n",
    "    # Get the file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "\n",
    "    # Convert the file size from bytes to MB\n",
    "    file_size_mb = file_size / 1048576\n",
    "\n",
    "    # Print the file name and size in MB\n",
    "    print(f'{file}: {file_size_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "499cba07-4615-4c66-bafe-b788e1293a85",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "\n",
    "pro_dir = './Congress'\n",
    "data_dir = \"./Congress/hein-bound/\"\n",
    "text_dir = \"./Congress/hein-bound_parsed/\"\n",
    "model_dir = \"./Congress/model/\"\n",
    "\n",
    "dfresults = pd.read_csv(os.path.join(pro_dir,'congressyear.csv'), dtype={'filenum': str} )\n",
    "\n",
    "# year = sys.argv[1]\n",
    "year = 1873\n",
    "model_file = os.path.join(model_dir,str(year)+'.model')\n",
    "if not os.path.exists(model_file):\n",
    "\tdf_new = dfresults.loc[dfresults['year']==year]\n",
    "\tallline = []\n",
    "\tfor filenum in list(set(df_new['filenum'])):\n",
    "\t\tprint(filenum)\n",
    "\t\tinfile = f'./Congress/hein-bound_parsed/speeches_{filenum}.jsonlist'\n",
    "\t\twith open(infile, 'r') as f:\n",
    "\t\t\t# Read the lines from the file\n",
    "\t\t\tlines = f.readlines()\n",
    "\n",
    "\t\t\tfor line in lines:\n",
    "\t\t\t\tjline = json.loads(line)\n",
    "\t\t\t\tif int(jline['id']) in df_new['speech_id'].to_list():\n",
    "\t\t\t\t\t# print(int(jline['id']))\n",
    "\t\t\t\t\t# print(jline['tokens'])\n",
    "\t\t\t\t\tallline.extend(jline['tokens'])\n",
    "\t# Set values for various parameters\n",
    "\tnum_features = 100    # Word vector dimensionality\n",
    "\tmin_word_count = 1    # Minimum word count\n",
    "\tnum_workers = 4       # Number of threads to run in parallel\n",
    "\tcontext = 10          # Context window size\n",
    "\tdownsampling = 1e-3   # Downsample setting for frequent words\n",
    "\tprint(allline[:3])\n",
    "# \tmodel_1 = Word2Vec(sentences = allline,\n",
    "# \t\t\t\t\t\tsg=1,\n",
    "# \t\t\t\t\t\tseed=1,\n",
    "# \t\t\t\t\t\tmin_count=min_word_count,\n",
    "# \t\t\t\t\t\twindow=context,\n",
    "# \t\t\t\t\t\tsample=downsampling)\n",
    "# \tmodel_1.save(model_file)\n",
    "\n",
    "# else:\n",
    "# \tprint(str(model_file), \" existed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a19d67-94ef-477b-b1b0-1836fcc1a84a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjline\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jline' is not defined"
     ]
    }
   ],
   "source": [
    "jline['id']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
